{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8eca2e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (0.8.41)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (from hdbscan) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (from hdbscan) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (from hdbscan) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (from hdbscan) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\이상엽\\desktop\\2026_snu_ai\\ml_project\\project2\\.venv\\lib\\site-packages (from scikit-learn>=1.6->hdbscan) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 필요 패키지 설치 (처음 1회만 실행)\n",
    "%pip install -q transformers torch\n",
    "%pip install hdbscan\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q pybloom-live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86955a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 2\n",
      "python-dotenv could not parse statement starting at line 3\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 5\n",
      "python-dotenv could not parse statement starting at line 6\n",
      "python-dotenv could not parse statement starting at line 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "설정 로드 완료\n",
      "================================================================================\n",
      "BERT 모델: bert-base-uncased\n",
      "클러스터링 Alpha: 0.5\n",
      "클러스터링 Method: dbscan\n",
      "Random State: 42\n",
      "Keep Prefixes: ('processName', 'eventName', 'syscall', 'args', 'executable', 'returnValue')\n",
      "Batch Size: 32\n",
      "Shuffle: False\n",
      "Top N Clusters: 10\n",
      "N-gram Size: 5\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 487.14it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# .env 파일에서 설정 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 설정값 가져오기\n",
    "BERT_MODEL_NAME = os.getenv('BERT_MODEL_NAME', 'bert-base-uncased')\n",
    "CLUSTERING_ALPHA = float(os.getenv('CLUSTERING_ALPHA', '1.0'))\n",
    "CLUSTERING_METHOD = os.getenv('CLUSTERING_METHOD', 'dbscan')\n",
    "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', '42'))\n",
    "KEEP_PREFIXES_STR = os.getenv('KEEP_PREFIXES', 'processName,eventName,syscall,args,executable,returnValue')\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '32'))\n",
    "SHUFFLE = os.getenv('SHUFFLE', 'False').lower() == 'true'\n",
    "TOP_N_CLUSTERS = int(os.getenv('TOP_N_CLUSTERS', '10'))\n",
    "N_GRAM_SIZE = int(os.getenv('N_GRAM_SIZE_CONF', '2'))\n",
    "BLOOM_FILTER_ERROR_RATE = float(os.getenv('BLOOM_FILTER_ERROR_RATE', '0.001'))\n",
    "\n",
    "# keep_prefixes를 튜플로 변환\n",
    "keep_prefixes = tuple([prefix.strip() for prefix in KEEP_PREFIXES_STR.split(',')])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"설정 로드 완료\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"BERT 모델: {BERT_MODEL_NAME}\")\n",
    "print(f\"클러스터링 Alpha: {CLUSTERING_ALPHA}\")\n",
    "print(f\"클러스터링 Method: {CLUSTERING_METHOD}\")\n",
    "print(f\"Random State: {CLUSTERING_RANDOM_STATE}\")\n",
    "print(f\"Keep Prefixes: {keep_prefixes}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Shuffle: {SHUFFLE}\")\n",
    "print(f\"Top N Clusters: {TOP_N_CLUSTERS}\")\n",
    "print(f\"N-gram Size: {N_GRAM_SIZE}\")\n",
    "print(f\"Bloom Filter Error Rate: {BLOOM_FILTER_ERROR_RATE} ({BLOOM_FILTER_ERROR_RATE*100:.3f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BERT 다운로드 및 로드\n",
    "extractor = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=BERT_MODEL_NAME,\n",
    "    tokenizer=BERT_MODEL_NAME,\n",
    "    return_tensors=True,\n",
    ")\n",
    "\n",
    "def mean_pool(token_embeddings):\n",
    "    # token_embeddings: (batch, seq_len, hidden_size)\n",
    "    return token_embeddings.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25221a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Literal\n",
    "\n",
    "class TraceeDataset(Dataset):\n",
    "    def __init__(self, data_path, filter_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: JSON Lines 파일 경로\n",
    "            filter_func: 텍스트 필터링 함수 (선택사항)\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.data_path = data_path  # 원본 데이터 경로 저장\n",
    "        self.filter_func = filter_func\n",
    "        self.original_data = []  # 원본 JSON 데이터 저장\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        json_obj = json.loads(line)\n",
    "                        # 원본 데이터 저장\n",
    "                        self.original_data.append(json_obj)\n",
    "                        # JSON 객체를 문자열로 변환\n",
    "                        text = json.dumps(json_obj, ensure_ascii=False)\n",
    "                        if filter_func:\n",
    "                            text = filter_func(text)\n",
    "                        self.data.append(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def getDataLoader(\n",
    "    split: Literal[\"train\", \"validation\"],\n",
    "    data_type: Literal[\"attack\", \"normal\"],\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    filter_func=None,\n",
    "    num_workers: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 로더를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        split: \"train\" 또는 \"validation\"\n",
    "        data_type: \"attack\" 또는 \"normal\"\n",
    "        batch_size: 배치 크기\n",
    "        shuffle: 셔플 여부 (기본값: True)\n",
    "        filter_func: 텍스트 필터링 함수 (선택사항)\n",
    "        num_workers: 데이터 로딩 워커 수 (기본값: 0)\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader 객체\n",
    "    \"\"\"\n",
    "    # 파일명 구성\n",
    "    prefix = \"tr\" if split == \"train\" else \"val\"\n",
    "    filename = f\"{prefix}_{data_type}_tracee.json\"\n",
    "    data_path = os.path.join(\"data\", filename)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"데이터 파일을 찾을 수 없습니다: {data_path}\")\n",
    "    \n",
    "    # Dataset 생성\n",
    "    dataset = TraceeDataset(data_path, filter_func=filter_func)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda x: x  # 배치를 리스트로 반환\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f14f8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 유사도를 높이는 공통/식별값(IDs, timestamp 등)을 제거하고 핵심 필드만 남깁니다.\n",
    "# keep_prefixes는 .env 파일에서 로드됨 (셀 1에서 설정됨)\n",
    "\n",
    "kv_pattern = re.compile(r'\"([^\"]+)\":\\s*\"([^\"]*)\"')\n",
    "\n",
    "def filter_text(raw_text, keep_keys=None):\n",
    "    \"\"\"\n",
    "    텍스트에서 지정된 키만 남기는 필터링 함수\n",
    "    \n",
    "    Args:\n",
    "        raw_text: 원본 JSON 문자열\n",
    "        keep_keys: 유지할 키의 튜플 (기본값: 전역 keep_prefixes)\n",
    "    \n",
    "    Returns:\n",
    "        필터링된 텍스트 문자열\n",
    "    \"\"\"\n",
    "    if keep_keys is None:\n",
    "        keep_keys = keep_prefixes\n",
    "    pairs = kv_pattern.findall(raw_text)\n",
    "    kept = []\n",
    "    for key, value in pairs:\n",
    "        if key.startswith(keep_keys):\n",
    "            kept.append(f\"{key}={value}\")\n",
    "    return \" \".join(kept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ced3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수: 721\n",
      "임베딩 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "임베딩 생성: 100%|██████████| 23/23 [01:31<00:00,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "완료! 임베딩 리스트 길이: 721\n",
      "임베딩 텐서 shape: torch.Size([721, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Train normal 데이터 로더 생성 (.env에서 설정값 사용)\n",
    "train_normal_loader = getDataLoader(\n",
    "    \"train\", \n",
    "    \"normal\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    filter_func=filter_text\n",
    ")\n",
    "\n",
    "# 모든 데이터를 임베딩으로 변환하여 리스트에 저장\n",
    "embeddings_list = []\n",
    "\n",
    "print(f\"총 데이터 개수: {len(train_normal_loader.dataset)}\")\n",
    "print(\"임베딩 생성 중...\")\n",
    "\n",
    "for batch_idx, batch_texts in enumerate(tqdm(train_normal_loader, desc=\"임베딩 생성\")):\n",
    "    # 배치 내 각 텍스트에 대해 임베딩 생성\n",
    "    for text in batch_texts:\n",
    "        # 문장 그대로 입력하면 내부에서 토크나이즈가 처리됨\n",
    "        outputs = extractor(text, padding=True, truncation=True)  # Shape: (1, seq_len, hidden_size)\n",
    "        # 평균 풀링하여 문장 임베딩 생성\n",
    "        pooled_embedding = mean_pool(outputs)  # Shape: (1, hidden_size)\n",
    "        # 배치 차원 제거하고 리스트에 추가\n",
    "        embeddings_list.append(pooled_embedding.squeeze(0))  # Shape: (hidden_size,)\n",
    "\n",
    "# 리스트를 텐서로 변환 (선택사항)\n",
    "embeddings_tensor = torch.stack(embeddings_list)  # Shape: (num_samples, hidden_size)\n",
    "\n",
    "print(f\"\\n완료! 임베딩 리스트 길이: {len(embeddings_list)}\")\n",
    "print(f\"임베딩 텐서 shape: {embeddings_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad2e4271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "클러스터 개수를 미리 지정하지 않는 알고리즘들 비교\n",
      "================================================================================\n",
      "\n",
      "[DBSCAN] - 밀도 기반, 노이즈 포인트 식별 가능\n",
      "--------------------------------------------------------------------------------\n",
      "Alpha: 0.5 -> 클러스터 수: 10, 노이즈: 13, 실루엣: 1.0000\n",
      "Alpha: 1.0 -> 클러스터 수: 7, 노이즈: 23, 실루엣: 1.0000\n",
      "Alpha: 1.5 -> 클러스터 수: 2, 노이즈: 448, 실루엣: 1.0000\n",
      "\n",
      "[HDBSCAN] - DBSCAN 개선 버전, 다양한 크기 클러스터 처리 우수 (추천!)\n",
      "--------------------------------------------------------------------------------\n",
      "Alpha: 0.5 -> 클러스터 수: 3, 노이즈: 56, 실루엣: 0.9076\n",
      "Alpha: 1.0 -> 클러스터 수: 3, 노이즈: 56, 실루엣: 0.9076\n",
      "Alpha: 1.5 -> 클러스터 수: 4, 노이즈: 3, 실루엣: 0.9120\n",
      "\n",
      "[Agglomerative Clustering] - 계층적, 거리 threshold 기반\n",
      "--------------------------------------------------------------------------------\n",
      "Alpha: 0.5 -> 클러스터 수: 8, 실루엣: 0.9584\n",
      "Alpha: 1.0 -> 클러스터 수: 12, 실루엣: 0.9782\n",
      "Alpha: 1.5 -> 클러스터 수: 14, 실루엣: 0.9840\n",
      "\n",
      "[Mean Shift] - 밀도 기반, bandwidth 조절\n",
      "--------------------------------------------------------------------------------\n",
      "Alpha: 0.5 -> 클러스터 수: 1, 실루엣: 0.0000\n",
      "Alpha: 1.0 -> 클러스터 수: 1, 실루엣: 0.0000\n",
      "Alpha: 1.5 -> 클러스터 수: 1, 실루엣: 0.0000\n",
      "\n",
      "================================================================================\n",
      "추천: 클러스터 개수가 많고 미리 지정할 수 없다면 HDBSCAN 또는 DBSCAN 사용\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# HDBSCAN은 별도 설치 필요: pip install hdbscan\n",
    "try:\n",
    "    import hdbscan\n",
    "    HDBSCAN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HDBSCAN_AVAILABLE = False\n",
    "    print(\"HDBSCAN을 사용하려면 'pip install hdbscan'을 실행하세요.\")\n",
    "\n",
    "def cluster_embeddings(embeddings, alpha=1.0, method='kmeans', random_state=42):\n",
    "    \"\"\"\n",
    "    임베딩을 클러스터링하는 함수\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 텐서 또는 numpy 배열 (n_samples, embedding_dim)\n",
    "        alpha: 클러스터링 비율 조절 파라미터 (0.1 ~ 2.0)\n",
    "              - alpha가 낮을수록 적은 클러스터 (큰 그룹)\n",
    "              - alpha가 높을수록 많은 클러스터 (세밀한 그룹)\n",
    "        method: 클러스터링 방법 ('kmeans' 또는 'dbscan')\n",
    "        random_state: 랜덤 시드\n",
    "    \n",
    "    Returns:\n",
    "        labels: 각 샘플의 클러스터 레이블 (numpy array)\n",
    "        n_clusters: 생성된 클러스터 수\n",
    "        cluster_info: 클러스터 정보 딕셔너리\n",
    "    \"\"\"\n",
    "    # 텐서를 numpy 배열로 변환\n",
    "    if isinstance(embeddings, torch.Tensor):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = np.array(embeddings)\n",
    "    \n",
    "    n_samples = embeddings_np.shape[0]\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        # alpha 값에 따라 클러스터 수 결정\n",
    "        # alpha=0.1 -> sqrt(n_samples/10), alpha=1.0 -> sqrt(n_samples), alpha=2.0 -> sqrt(n_samples*10)\n",
    "        base_clusters = int(np.sqrt(n_samples))\n",
    "        n_clusters = max(2, int(base_clusters * alpha))\n",
    "        \n",
    "        # K-means 클러스터링\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 실루엣 점수 계산\n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'kmeans',\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'inertia': kmeans.inertia_,\n",
    "            'centers': kmeans.cluster_centers_\n",
    "        }\n",
    "        \n",
    "    elif method == 'dbscan':\n",
    "        # alpha 값에 따라 eps와 min_samples 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 eps), 높을수록 작은 클러스터 (작은 eps)\n",
    "        # 기본 eps는 데이터의 평균 거리의 일정 비율\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        neighbors = NearestNeighbors(n_neighbors=min(10, n_samples))\n",
    "        neighbors_fit = neighbors.fit(embeddings_np)\n",
    "        distances, indices = neighbors_fit.kneighbors(embeddings_np)\n",
    "        distances = np.sort(distances, axis=0)\n",
    "        distances = distances[:, 1]\n",
    "        \n",
    "        # 기본 eps는 거리의 중앙값\n",
    "        base_eps = np.median(distances)\n",
    "        # alpha에 따라 조절: alpha=0.1 -> 큰 eps, alpha=2.0 -> 작은 eps\n",
    "        eps = base_eps / alpha\n",
    "        \n",
    "        min_samples = max(3, int(np.log(n_samples) * alpha))\n",
    "        \n",
    "        # DBSCAN 클러스터링\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산 (노이즈 제외)\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # 실루엣 점수 계산 (노이즈 제외)\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np[mask], labels[mask])\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'dbscan',\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples\n",
    "        }\n",
    "    \n",
    "    elif method == 'hdbscan':\n",
    "        # HDBSCAN: DBSCAN의 개선 버전, 클러스터 개수 자동 결정\n",
    "        # 밀도 기반 계층적 클러스터링, 다양한 크기의 클러스터 처리에 우수\n",
    "        if not HDBSCAN_AVAILABLE:\n",
    "            raise ImportError(\"HDBSCAN이 설치되지 않았습니다. 'pip install hdbscan'을 실행하세요.\")\n",
    "        \n",
    "        # alpha 값에 따라 min_cluster_size와 min_samples 조절\n",
    "        # alpha가 낮을수록 큰 클러스터, 높을수록 작은 클러스터\n",
    "        min_cluster_size = max(2, int(n_samples / (10 * alpha)))\n",
    "        min_samples = max(3, int(np.log(n_samples) * alpha))\n",
    "        \n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=0.0\n",
    "        )\n",
    "        labels = clusterer.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산 (노이즈 제외)\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # 실루엣 점수 계산 (노이즈 제외)\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np[mask], labels[mask])\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'hdbscan',\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'min_samples': min_samples,\n",
    "            'cluster_probabilities': clusterer.probabilities_\n",
    "        }\n",
    "    \n",
    "    elif method == 'agglomerative':\n",
    "        # Agglomerative Clustering: 거리 threshold로 클러스터 개수 자동 결정\n",
    "        # 계층적 클러스터링, 거리 기반으로 클러스터 수 자동 결정\n",
    "        \n",
    "        # 거리 행렬 계산\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        distances = pairwise_distances(embeddings_np)\n",
    "        \n",
    "        # alpha 값에 따라 거리 threshold 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 threshold), 높을수록 작은 클러스터 (작은 threshold)\n",
    "        base_threshold = np.percentile(distances[distances > 0], 50)  # 중앙값\n",
    "        distance_threshold = base_threshold / alpha\n",
    "        \n",
    "        # Agglomerative Clustering\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=distance_threshold,\n",
    "            linkage='ward'\n",
    "        )\n",
    "        labels = clustering.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels)\n",
    "        \n",
    "        # 실루엣 점수 계산\n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'agglomerative',\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'distance_threshold': distance_threshold\n",
    "        }\n",
    "    \n",
    "    elif method == 'meanshift':\n",
    "        # Mean Shift: 클러스터 개수 자동 결정, 밀도 기반\n",
    "        # bandwidth(대역폭)에 따라 클러스터 수 결정\n",
    "        \n",
    "        # alpha 값에 따라 bandwidth 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 bandwidth), 높을수록 작은 클러스터 (작은 bandwidth)\n",
    "        from sklearn.cluster import estimate_bandwidth\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        # 거리 기반으로 bandwidth 추정 (더 안정적)\n",
    "        try:\n",
    "            # 샘플링하여 거리 계산 (대용량 데이터 처리)\n",
    "            sample_size = min(500, n_samples)\n",
    "            if n_samples > sample_size:\n",
    "                sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "                sample_data = embeddings_np[sample_indices]\n",
    "            else:\n",
    "                sample_data = embeddings_np\n",
    "            \n",
    "            # 거리 행렬 계산\n",
    "            distances = pairwise_distances(sample_data)\n",
    "            # 0이 아닌 거리의 중앙값 사용\n",
    "            non_zero_distances = distances[distances > 0]\n",
    "            if len(non_zero_distances) > 0:\n",
    "                base_bandwidth = np.median(non_zero_distances)\n",
    "            else:\n",
    "                # fallback: estimate_bandwidth 사용\n",
    "                base_bandwidth = estimate_bandwidth(embeddings_np, quantile=0.3, n_samples=sample_size)\n",
    "        except:\n",
    "            # fallback: estimate_bandwidth 사용\n",
    "            base_bandwidth = estimate_bandwidth(embeddings_np, quantile=0.3, n_samples=min(500, n_samples))\n",
    "        \n",
    "        # alpha에 따라 조절: alpha가 낮을수록 큰 bandwidth\n",
    "        # 최소 bandwidth 보장 (너무 작으면 에러 발생)\n",
    "        bandwidth = max(base_bandwidth / alpha, base_bandwidth * 0.1)\n",
    "        \n",
    "        # Mean Shift 클러스터링 (에러 처리 포함)\n",
    "        try:\n",
    "            ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, max_iter=300)\n",
    "            labels = ms.fit_predict(embeddings_np)\n",
    "            \n",
    "            # 클러스터 수 계산\n",
    "            unique_labels = set(labels)\n",
    "            n_clusters = len(unique_labels)\n",
    "            \n",
    "            # 실루엣 점수 계산\n",
    "            if n_clusters > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "            \n",
    "            cluster_info = {\n",
    "                'method': 'meanshift',\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': silhouette_avg,\n",
    "                'bandwidth': bandwidth\n",
    "            }\n",
    "        except ValueError as e:\n",
    "            # bandwidth가 너무 작아서 에러 발생 시, 자동으로 증가시켜 재시도\n",
    "            if \"bandwidth\" in str(e).lower():\n",
    "                # bandwidth를 2배로 증가\n",
    "                bandwidth = bandwidth * 2\n",
    "                try:\n",
    "                    ms = MeanShift(bandwidth=bandwidth, bin_seeding=False, max_iter=300)\n",
    "                    labels = ms.fit_predict(embeddings_np)\n",
    "                    unique_labels = set(labels)\n",
    "                    n_clusters = len(unique_labels)\n",
    "                    if n_clusters > 1:\n",
    "                        silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "                    else:\n",
    "                        silhouette_avg = 0.0\n",
    "                    cluster_info = {\n",
    "                        'method': 'meanshift',\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'silhouette_score': silhouette_avg,\n",
    "                        'bandwidth': bandwidth,\n",
    "                        'note': 'bandwidth 자동 조정됨'\n",
    "                    }\n",
    "                except:\n",
    "                    # 여전히 실패하면 모든 포인트를 하나의 클러스터로\n",
    "                    labels = np.zeros(n_samples, dtype=int)\n",
    "                    cluster_info = {\n",
    "                        'method': 'meanshift',\n",
    "                        'n_clusters': 1,\n",
    "                        'silhouette_score': 0.0,\n",
    "                        'bandwidth': bandwidth,\n",
    "                        'note': 'bandwidth 조정 실패, 모든 포인트를 하나의 클러스터로 처리'\n",
    "                    }\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 클러스터링 방법: {method}. \"\n",
    "                        f\"사용 가능한 방법: 'kmeans', 'dbscan', 'hdbscan', 'agglomerative', 'meanshift'\")\n",
    "    \n",
    "    return labels, n_clusters, cluster_info\n",
    "\n",
    "# 사용 예시 및 알고리즘 비교\n",
    "if 'embeddings_tensor' in globals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"클러스터 개수를 미리 지정하지 않는 알고리즘들 비교\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # DBSCAN 테스트\n",
    "    print(\"\\n[DBSCAN] - 밀도 기반, 노이즈 포인트 식별 가능\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='dbscan'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 노이즈: {info['n_noise']}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # HDBSCAN 테스트 (가장 추천!)\n",
    "    if HDBSCAN_AVAILABLE:\n",
    "        print(\"\\n[HDBSCAN] - DBSCAN 개선 버전, 다양한 크기 클러스터 처리 우수 (추천!)\")\n",
    "        print(\"-\" * 80)\n",
    "        for alpha in [0.5, 1.0, 1.5]:\n",
    "            try:\n",
    "                labels, n_clusters, info = cluster_embeddings(\n",
    "                    embeddings_tensor, \n",
    "                    alpha=alpha, \n",
    "                    method='hdbscan'\n",
    "                )\n",
    "                print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 노이즈: {info['n_noise']}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Alpha: {alpha:.1f} -> 오류: {e}\")\n",
    "    else:\n",
    "        print(\"\\n[HDBSCAN] - 설치 필요: pip install hdbscan\")\n",
    "    \n",
    "    # Agglomerative Clustering 테스트\n",
    "    print(\"\\n[Agglomerative Clustering] - 계층적, 거리 threshold 기반\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='agglomerative'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # Mean Shift 테스트\n",
    "    print(\"\\n[Mean Shift] - 밀도 기반, bandwidth 조절\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='meanshift'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"추천: 클러스터 개수가 많고 미리 지정할 수 없다면 HDBSCAN 또는 DBSCAN 사용\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"embeddings_tensor가 아직 생성되지 않았습니다. 먼저 셀 4를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03f92399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "class ClusterTracer:\n",
    "    \"\"\"\n",
    "    클러스터링 결과와 원본 데이터를 매칭하여 추적하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, cluster_labels, filter_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataloader: 데이터 로더 (원본 데이터 접근용)\n",
    "            cluster_labels: 클러스터링 결과 레이블 (numpy array)\n",
    "            filter_func: 필터링 함수 (원본 데이터 로드 시 사용)\n",
    "        \"\"\"\n",
    "        self.dataloader = dataloader\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.filter_func = filter_func\n",
    "        self.original_data = []\n",
    "        self.cluster_to_indices = defaultdict(list)\n",
    "        \n",
    "        # 원본 데이터 로드\n",
    "        self._load_original_data()\n",
    "        \n",
    "        # 클러스터별 인덱스 매핑\n",
    "        self._map_clusters()\n",
    "    \n",
    "    def _load_original_data(self):\n",
    "        \"\"\"원본 JSON 데이터를 로드\"\"\"\n",
    "        dataset = self.dataloader.dataset\n",
    "        \n",
    "        # TraceeDataset에서 원본 데이터 가져오기\n",
    "        if hasattr(dataset, 'original_data'):\n",
    "            self.original_data = dataset.original_data\n",
    "        elif hasattr(dataset, 'data_path'):\n",
    "            # 원본 파일에서 다시 로드\n",
    "            data_path = dataset.data_path\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            json_obj = json.loads(line)\n",
    "                            self.original_data.append(json_obj)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        else:\n",
    "            # 데이터 로더에서 직접 가져오기 (필터링된 버전)\n",
    "            for i in range(len(dataset)):\n",
    "                text = dataset[i]\n",
    "                # 필터링된 텍스트를 저장\n",
    "                self.original_data.append({'filtered_text': text})\n",
    "    \n",
    "    def _map_clusters(self):\n",
    "        \"\"\"클러스터 레이블과 데이터 인덱스를 매핑\"\"\"\n",
    "        for idx, label in enumerate(self.cluster_labels):\n",
    "            self.cluster_to_indices[label].append(idx)\n",
    "    \n",
    "    def get_cluster_data(self, cluster_id):\n",
    "        \"\"\"\n",
    "        특정 클러스터에 속한 모든 데이터 반환\n",
    "        \n",
    "        Args:\n",
    "            cluster_id: 클러스터 ID\n",
    "        \n",
    "        Returns:\n",
    "            해당 클러스터에 속한 데이터 리스트\n",
    "        \"\"\"\n",
    "        if cluster_id not in self.cluster_to_indices:\n",
    "            return []\n",
    "        \n",
    "        indices = self.cluster_to_indices[cluster_id]\n",
    "        return [self.original_data[idx] for idx in indices]\n",
    "    \n",
    "    def get_cluster_summary(self):\n",
    "        \"\"\"\n",
    "        클러스터별 요약 정보 반환\n",
    "        \n",
    "        Returns:\n",
    "            클러스터 요약 정보 딕셔너리\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            summary[cluster_id] = {\n",
    "                'count': len(indices),\n",
    "                'indices': indices,\n",
    "                'sample_data': self.original_data[indices[0]] if indices else None\n",
    "            }\n",
    "        return summary\n",
    "    \n",
    "    def print_cluster_summary(self, top_n=10):\n",
    "        \"\"\"\n",
    "        클러스터 요약 정보 출력\n",
    "        \n",
    "        Args:\n",
    "            top_n: 출력할 상위 클러스터 개수\n",
    "        \"\"\"\n",
    "        # 클러스터를 크기 순으로 정렬\n",
    "        sorted_clusters = sorted(\n",
    "            self.cluster_to_indices.items(),\n",
    "            key=lambda x: len(x[1]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"클러스터 요약 정보\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"총 클러스터 수: {len(self.cluster_to_indices)}\")\n",
    "        print(f\"총 데이터 수: {len(self.cluster_labels)}\")\n",
    "        print()\n",
    "        \n",
    "        for i, (cluster_id, indices) in enumerate(sorted_clusters[:top_n]):\n",
    "            print(f\"[클러스터 {cluster_id}] 데이터 개수: {len(indices)}\")\n",
    "            if indices:\n",
    "                sample = self.original_data[indices[0]]\n",
    "                # 주요 필드만 출력\n",
    "                if isinstance(sample, dict):\n",
    "                    key_fields = ['processName', 'eventName', 'syscall', 'eventId']\n",
    "                    sample_info = {k: sample.get(k, 'N/A') for k in key_fields if k in sample}\n",
    "                    print(f\"  샘플 데이터: {sample_info}\")\n",
    "                else:\n",
    "                    print(f\"  샘플 데이터: {str(sample)[:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    def get_cluster_statistics(self):\n",
    "        \"\"\"\n",
    "        클러스터 통계 정보 반환 (DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "            클러스터 통계 DataFrame\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            cluster_data = [self.original_data[idx] for idx in indices]\n",
    "            \n",
    "            # 주요 필드별 통계\n",
    "            process_names = [d.get('processName', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            event_names = [d.get('eventName', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            syscalls = [d.get('syscall', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            \n",
    "            stats.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'count': len(indices),\n",
    "                'unique_processes': len(set(process_names)),\n",
    "                'unique_events': len(set(event_names)),\n",
    "                'unique_syscalls': len(set(syscalls)),\n",
    "                'most_common_process': max(set(process_names), key=process_names.count) if process_names else 'N/A',\n",
    "                'most_common_event': max(set(event_names), key=event_names.count) if event_names else 'N/A',\n",
    "                'most_common_syscall': max(set(syscalls), key=syscalls.count) if syscalls else 'N/A',\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).sort_values('count', ascending=False)\n",
    "    \n",
    "    def export_cluster_mapping(self, output_path='cluster_mapping.json'):\n",
    "        \"\"\"\n",
    "        클러스터 매핑 정보를 JSON 파일로 저장\n",
    "        각 인덱스에 번호와 실제 텍스트 데이터를 포함\n",
    "        \n",
    "        Args:\n",
    "            output_path: 출력 파일 경로\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            'total_clusters': len(self.cluster_to_indices),\n",
    "            'total_samples': len(self.cluster_labels),\n",
    "            'clusters': {}\n",
    "        }\n",
    "        \n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            # 각 인덱스에 대해 번호와 실제 데이터를 매핑\n",
    "            indexed_data = []\n",
    "            for idx in indices:\n",
    "                data = self.original_data[idx]\n",
    "                indexed_data.append({\n",
    "                    'index': idx,\n",
    "                    'data': data\n",
    "                })\n",
    "            \n",
    "            mapping['clusters'][str(cluster_id)] = {\n",
    "                'count': len(indices),\n",
    "                'indices': indices,  # 기존 인덱스 리스트도 유지\n",
    "                'data': indexed_data,  # 번호와 실제 데이터 매핑\n",
    "                'sample_data': self.original_data[indices[0]] if indices else None\n",
    "            }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"클러스터 매핑 정보가 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95ad4db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터링 수행 중...\n",
      "\n",
      "클러스터링 완료!\n",
      "클러스터 수: 10\n",
      "노이즈 포인트: 13\n",
      "실루엣 점수: 1.0000\n",
      "\n",
      "================================================================================\n",
      "클러스터 추적 객체 생성 중...\n",
      "================================================================================\n",
      "클러스터 요약 정보\n",
      "================================================================================\n",
      "총 클러스터 수: 11\n",
      "총 데이터 수: 721\n",
      "\n",
      "[클러스터 8] 데이터 개수: 258\n",
      "  샘플 데이터: {'processName': 'init', 'eventName': 'setresgid', 'syscall': 'setresgid', 'eventId': '119'}\n",
      "\n",
      "[클러스터 9] 데이터 개수: 258\n",
      "  샘플 데이터: {'processName': 'init', 'eventName': 'setresuid', 'syscall': 'setresuid', 'eventId': '117'}\n",
      "\n",
      "[클러스터 0] 데이터 개수: 62\n",
      "  샘플 데이터: {'processName': 'initd', 'eventName': 'security_socket_connect', 'syscall': 'connect', 'eventId': '736'}\n",
      "\n",
      "[클러스터 2] 데이터 개수: 53\n",
      "  샘플 데이터: {'processName': 'bash', 'eventName': 'setpgid', 'syscall': 'setpgid', 'eventId': '109'}\n",
      "\n",
      "[클러스터 5] 데이터 개수: 36\n",
      "  샘플 데이터: {'processName': 'ls', 'eventName': 'security_socket_connect', 'syscall': 'connect', 'eventId': '736'}\n",
      "\n",
      "[클러스터 1] 데이터 개수: 16\n",
      "  샘플 데이터: {'processName': 'docker-desktop-', 'eventName': 'security_socket_connect', 'syscall': 'connect', 'eventId': '736'}\n",
      "\n",
      "[클러스터 3] 데이터 개수: 15\n",
      "  샘플 데이터: {'processName': 'ls', 'eventName': 'sched_process_exec', 'syscall': 'execve', 'eventId': '715'}\n",
      "\n",
      "[클러스터 -1] 데이터 개수: 13\n",
      "  샘플 데이터: {'processName': 'tracee', 'eventName': 'security_socket_connect', 'syscall': 'connect', 'eventId': '736'}\n",
      "\n",
      "[클러스터 7] 데이터 개수: 4\n",
      "  샘플 데이터: {'processName': 'init', 'eventName': 'security_socket_connect', 'syscall': 'connect', 'eventId': '736'}\n",
      "\n",
      "[클러스터 4] 데이터 개수: 3\n",
      "  샘플 데이터: {'processName': 'systemd-timesyn', 'eventName': 'security_socket_bind', 'syscall': 'bind', 'eventId': '738'}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "클러스터 통계 정보\n",
      "================================================================================\n",
      "    cluster_id  count  unique_processes  unique_events  unique_syscalls  \\\n",
      "9            8    258                 1              1                1   \n",
      "10           9    258                 1              1                1   \n",
      "0            0     62                 1              1                1   \n",
      "3            2     53                 1              1                1   \n",
      "6            5     36                 1              1                1   \n",
      "2            1     16                 1              1                1   \n",
      "4            3     15                 1              1                1   \n",
      "1           -1     13                 8              5                4   \n",
      "8            7      4                 1              1                1   \n",
      "5            4      3                 1              1                1   \n",
      "7            6      3                 1              1                1   \n",
      "\n",
      "   most_common_process        most_common_event most_common_syscall  \n",
      "9                 init                setresgid           setresgid  \n",
      "10                init                setresuid           setresuid  \n",
      "0                initd  security_socket_connect             connect  \n",
      "3                 bash                  setpgid             setpgid  \n",
      "6                   ls  security_socket_connect             connect  \n",
      "2      docker-desktop-  security_socket_connect             connect  \n",
      "4                   ls       sched_process_exec              execve  \n",
      "1                   vi  security_socket_connect             connect  \n",
      "8                 init  security_socket_connect             connect  \n",
      "5      systemd-timesyn     security_socket_bind                bind  \n",
      "7        systemd-udevd    security_inode_unlink              unlink  \n",
      "\n",
      "================================================================================\n",
      "특정 클러스터 데이터 확인 예시\n",
      "================================================================================\n",
      "\n",
      "가장 큰 클러스터 ID: 8\n",
      "데이터 개수: 258\n",
      "\n",
      "첫 3개 샘플:\n",
      "\n",
      "샘플 1:\n",
      "  processName: init\n",
      "  eventName: setresgid\n",
      "  syscall: setresgid\n",
      "  eventId: 119\n",
      "  timestamp: 1770361192897650132\n",
      "\n",
      "샘플 2:\n",
      "  processName: init\n",
      "  eventName: setresgid\n",
      "  syscall: setresgid\n",
      "  eventId: 119\n",
      "  timestamp: 1770361192897682175\n",
      "\n",
      "샘플 3:\n",
      "  processName: init\n",
      "  eventName: setresgid\n",
      "  syscall: setresgid\n",
      "  eventId: 119\n",
      "  timestamp: 1770361192898121207\n",
      "클러스터 매핑 정보가 cluster_mapping.json에 저장되었습니다.\n",
      "\n",
      "================================================================================\n",
      "사용 가능한 메서드:\n",
      "================================================================================\n",
      "1. tracer.get_cluster_data(cluster_id) - 특정 클러스터의 모든 데이터 반환\n",
      "2. tracer.get_cluster_summary() - 클러스터별 요약 정보 딕셔너리\n",
      "3. tracer.print_cluster_summary(top_n=10) - 클러스터 요약 정보 출력\n",
      "4. tracer.get_cluster_statistics() - 클러스터 통계 DataFrame\n",
      "5. tracer.export_cluster_mapping(path) - 클러스터 매핑 정보를 JSON으로 저장\n"
     ]
    }
   ],
   "source": [
    "# 클러스터링 결과와 원본 데이터 매칭 예시\n",
    "if 'embeddings_tensor' in globals() and 'train_normal_loader' in globals():\n",
    "    # 클러스터링 수행 (.env에서 설정값 사용)\n",
    "    print(\"클러스터링 수행 중...\")\n",
    "    labels, n_clusters, info = cluster_embeddings(\n",
    "        embeddings_tensor,\n",
    "        alpha=CLUSTERING_ALPHA,\n",
    "        method=CLUSTERING_METHOD,\n",
    "        random_state=CLUSTERING_RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n클러스터링 완료!\")\n",
    "    print(f\"클러스터 수: {n_clusters}\")\n",
    "    print(f\"노이즈 포인트: {info.get('n_noise', 0)}\")\n",
    "    print(f\"실루엣 점수: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # ClusterTracer 생성\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"클러스터 추적 객체 생성 중...\")\n",
    "    tracer = ClusterTracer(\n",
    "        dataloader=train_normal_loader,\n",
    "        cluster_labels=labels,\n",
    "        filter_func=filter_text\n",
    "    )\n",
    "    \n",
    "    # 클러스터 요약 정보 출력 (.env에서 설정값 사용)\n",
    "    tracer.print_cluster_summary(top_n=TOP_N_CLUSTERS)\n",
    "    \n",
    "    # 클러스터 통계 정보 (DataFrame)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"클러스터 통계 정보\")\n",
    "    print(\"=\" * 80)\n",
    "    stats_df = tracer.get_cluster_statistics()\n",
    "    print(stats_df.head(15))\n",
    "    \n",
    "    # 특정 클러스터의 데이터 확인 예시\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"특정 클러스터 데이터 확인 예시\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 가장 큰 클러스터 확인\n",
    "    largest_cluster_id = stats_df.iloc[0]['cluster_id']\n",
    "    print(f\"\\n가장 큰 클러스터 ID: {largest_cluster_id}\")\n",
    "    cluster_data = tracer.get_cluster_data(largest_cluster_id)\n",
    "    print(f\"데이터 개수: {len(cluster_data)}\")\n",
    "    print(\"\\n첫 3개 샘플:\")\n",
    "    for i, data in enumerate(cluster_data[:3]):\n",
    "        print(f\"\\n샘플 {i+1}:\")\n",
    "        if isinstance(data, dict):\n",
    "            key_fields = ['processName', 'eventName', 'syscall', 'eventId', 'timestamp']\n",
    "            for field in key_fields:\n",
    "                if field in data:\n",
    "                    print(f\"  {field}: {data[field]}\")\n",
    "    \n",
    "    # 클러스터 매핑 정보 저장\n",
    "    tracer.export_cluster_mapping('cluster_mapping.json')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"사용 가능한 메서드:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. tracer.get_cluster_data(cluster_id) - 특정 클러스터의 모든 데이터 반환\")\n",
    "    print(\"2. tracer.get_cluster_summary() - 클러스터별 요약 정보 딕셔너리\")\n",
    "    print(\"3. tracer.print_cluster_summary(top_n=10) - 클러스터 요약 정보 출력\")\n",
    "    print(\"4. tracer.get_cluster_statistics() - 클러스터 통계 DataFrame\")\n",
    "    print(\"5. tracer.export_cluster_mapping(path) - 클러스터 매핑 정보를 JSON으로 저장\")\n",
    "    \n",
    "else:\n",
    "    print(\"embeddings_tensor 또는 train_normal_loader가 아직 생성되지 않았습니다.\")\n",
    "    print(\"먼저 셀 4를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aab12b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "N-gram 시퀀스 생성\n",
      "================================================================================\n",
      "\n",
      "1. 5-gram 생성 중...\n",
      "   총 717개의 5-gram 생성됨\n",
      "   클러스터 시퀀스 길이: 721\n",
      "\n",
      "2. 5-gram 통계:\n",
      "--------------------------------------------------------------------------------\n",
      "   고유 5-gram 수: 124\n",
      "\n",
      "   상위 10개 빈도 5-gram:\n",
      "    1. (5, 6, 6, 5, 5):  125회 (17.43%)\n",
      "    2. (6, 6, 5, 5, 6):  125회 (17.43%)\n",
      "    3. (6, 5, 5, 6, 6):  125회 (17.43%)\n",
      "    4. (5, 5, 6, 6, 5):  125회 (17.43%)\n",
      "    5. (0, 0, 0, 0, 0):   18회 (2.51%)\n",
      "    6. (2, 2, 2, 2, 2):   14회 (1.95%)\n",
      "    7. (2, 2, 3, 4, 4):    9회 (1.26%)\n",
      "    8. (2, 3, 4, 4, 4):    9회 (1.26%)\n",
      "    9. (3, 4, 4, 4, 4):    9회 (1.26%)\n",
      "   10. (0, 0, 0, 2, 2):    4회 (0.56%)\n",
      "\n",
      "3. 추가 통계 (Bigram, Trigram):\n",
      "--------------------------------------------------------------------------------\n",
      "   Bigram: 총 720개, 고유 56개\n",
      "   Trigram: 총 719개, 고유 81개\n",
      "\n",
      "4. 클러스터 시퀀스 샘플 (처음 20개):\n",
      "--------------------------------------------------------------------------------\n",
      "   [0, 7, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 8, 0, 0]\n",
      "\n",
      "5. 5-gram 샘플 (처음 10개):\n",
      "--------------------------------------------------------------------------------\n",
      "    1. (0, 7, 1, 1, 1)\n",
      "    2. (7, 1, 1, 1, 0)\n",
      "    3. (1, 1, 1, 0, 1)\n",
      "    4. (1, 1, 0, 1, 1)\n",
      "    5. (1, 0, 1, 1, 1)\n",
      "    6. (0, 1, 1, 1, 1)\n",
      "    7. (1, 1, 1, 1, 0)\n",
      "    8. (1, 1, 1, 0, 1)\n",
      "    9. (1, 1, 0, 1, 1)\n",
      "   10. (1, 0, 1, 1, 1)\n",
      "\n",
      "================================================================================\n",
      "N-gram 생성 완료!\n",
      "================================================================================\n",
      "\n",
      "사용 가능한 변수:\n",
      "  - ngrams: 717개의 5-gram (기본값, .env에서 설정)\n",
      "  - bigrams: 720개의 bigram\n",
      "  - trigrams: 719개의 trigram\n",
      "  - cluster_seq: 원본 클러스터 시퀀스 (721개)\n",
      "  - ngram_stats: 5-gram 통계 정보\n",
      "  - bigram_stats: bigram 통계 정보\n",
      "  - trigram_stats: trigram 통계 정보\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def create_ngram_sequence(matched_sequence_file='matched_sequence.json', n=None):\n",
    "    \"\"\"\n",
    "    matched_sequence.json에서 cluster_id를 추출하여 n-gram 시퀀스를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        matched_sequence_file: matched_sequence.json 파일 경로\n",
    "        n: n-gram의 n 값 (예: 2=bigram, 3=trigram)\n",
    "            None이면 전역 N_GRAM_SIZE 사용 (.env에서 로드)\n",
    "    \n",
    "    Returns:\n",
    "        list: n-gram 시퀀스 리스트 (각 항목은 n개의 cluster_id 튜플)\n",
    "    \"\"\"\n",
    "    # n 값이 없으면 전역 N_GRAM_SIZE 사용\n",
    "    if n is None:\n",
    "        n = globals().get('N_GRAM_SIZE', 2)\n",
    "    \n",
    "    # matched_sequence.json 파일 읽기\n",
    "    with open(matched_sequence_file, 'r', encoding='utf-8') as f:\n",
    "        matched_sequence = json.load(f)\n",
    "    \n",
    "    # cluster_id 시퀀스 추출 (인덱스 순서대로 정렬)\n",
    "    cluster_sequence = []\n",
    "    for item in sorted(matched_sequence, key=lambda x: x['index']):\n",
    "        cluster_sequence.append(item['cluster_id'])\n",
    "    \n",
    "    # n-gram 생성\n",
    "    ngrams = []\n",
    "    for i in range(len(cluster_sequence) - n + 1):\n",
    "        ngram = tuple(cluster_sequence[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams, cluster_sequence\n",
    "\n",
    "def analyze_ngrams(ngrams, top_k=10):\n",
    "    \"\"\"\n",
    "    n-gram 통계 분석\n",
    "    \n",
    "    Args:\n",
    "        ngrams: n-gram 리스트\n",
    "        top_k: 출력할 상위 k개 n-gram\n",
    "    \n",
    "    Returns:\n",
    "        dict: 통계 정보\n",
    "    \"\"\"\n",
    "    # n-gram 빈도 계산\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    # 통계 정보\n",
    "    stats = {\n",
    "        'total_ngrams': len(ngrams),\n",
    "        'unique_ngrams': len(ngram_counts),\n",
    "        'top_ngrams': ngram_counts.most_common(top_k),\n",
    "        'ngram_counts': dict(ngram_counts)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# n-gram 시퀀스 생성 (예시: bigram, trigram)\n",
    "print(\"=\" * 80)\n",
    "print(\"N-gram 시퀀스 생성\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# N-gram 생성 (.env에서 설정된 N_GRAM_SIZE 사용)\n",
    "print(f\"\\n1. {N_GRAM_SIZE}-gram 생성 중...\")\n",
    "ngrams, cluster_seq = create_ngram_sequence('matched_sequence.json', n=None)  # None이면 N_GRAM_SIZE 사용\n",
    "print(f\"   총 {len(ngrams)}개의 {N_GRAM_SIZE}-gram 생성됨\")\n",
    "print(f\"   클러스터 시퀀스 길이: {len(cluster_seq)}\")\n",
    "\n",
    "# N-gram 통계\n",
    "print(f\"\\n2. {N_GRAM_SIZE}-gram 통계:\")\n",
    "print(\"-\" * 80)\n",
    "ngram_stats = analyze_ngrams(ngrams, top_k=10)\n",
    "print(f\"   고유 {N_GRAM_SIZE}-gram 수: {ngram_stats['unique_ngrams']}\")\n",
    "print(f\"\\n   상위 10개 빈도 {N_GRAM_SIZE}-gram:\")\n",
    "for i, (ngram, count) in enumerate(ngram_stats['top_ngrams'], 1):\n",
    "    print(f\"   {i:2d}. {ngram}: {count:4d}회 ({count/len(ngrams)*100:.2f}%)\")\n",
    "\n",
    "# 추가: Bigram과 Trigram도 생성 (비교용)\n",
    "print(\"\\n3. 추가 통계 (Bigram, Trigram):\")\n",
    "print(\"-\" * 80)\n",
    "bigrams, _ = create_ngram_sequence('matched_sequence.json', n=2)\n",
    "trigrams, _ = create_ngram_sequence('matched_sequence.json', n=3)\n",
    "\n",
    "bigram_stats = analyze_ngrams(bigrams, top_k=5)\n",
    "trigram_stats = analyze_ngrams(trigrams, top_k=5)\n",
    "\n",
    "print(f\"   Bigram: 총 {len(bigrams)}개, 고유 {bigram_stats['unique_ngrams']}개\")\n",
    "print(f\"   Trigram: 총 {len(trigrams)}개, 고유 {trigram_stats['unique_ngrams']}개\")\n",
    "\n",
    "# 클러스터 시퀀스 샘플 출력\n",
    "print(f\"\\n4. 클러스터 시퀀스 샘플 (처음 20개):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   {cluster_seq[:20]}\")\n",
    "\n",
    "# N-gram 샘플 출력\n",
    "print(f\"\\n5. {N_GRAM_SIZE}-gram 샘플 (처음 10개):\")\n",
    "print(\"-\" * 80)\n",
    "for i, ngram in enumerate(ngrams[:10], 1):\n",
    "    print(f\"   {i:2d}. {ngram}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"N-gram 생성 완료!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n사용 가능한 변수:\")\n",
    "print(f\"  - ngrams: {len(ngrams)}개의 {N_GRAM_SIZE}-gram (기본값, .env에서 설정)\")\n",
    "print(f\"  - bigrams: {len(bigrams)}개의 bigram\")\n",
    "print(f\"  - trigrams: {len(trigrams)}개의 trigram\")\n",
    "print(f\"  - cluster_seq: 원본 클러스터 시퀀스 ({len(cluster_seq)}개)\")\n",
    "print(f\"  - ngram_stats: {N_GRAM_SIZE}-gram 통계 정보\")\n",
    "print(f\"  - bigram_stats: bigram 통계 정보\")\n",
    "print(f\"  - trigram_stats: trigram 통계 정보\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c101f291",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pybloom'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpybloom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BloomFilter\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_ngram_bloom_filter\u001b[39m(ngrams, error_rate=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pybloom'"
     ]
    }
   ],
   "source": [
    "from pybloom import BloomFilter\n",
    "import json\n",
    "\n",
    "def create_ngram_bloom_filter(ngrams, error_rate=None):\n",
    "    \"\"\"\n",
    "    N-gram 리스트로부터 Bloom filter를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        ngrams: n-gram 리스트 (튜플들의 리스트)\n",
    "        error_rate: False positive 확률 (None이면 전역 BLOOM_FILTER_ERROR_RATE 사용)\n",
    "                    기본값: 0.001 = 0.1%\n",
    "                    범위: 0 < error_rate < 1\n",
    "                    주의: 매우 작은 값(예: 1e-100)은 메모리 사용량을 극도로 증가시킬 수 있음\n",
    "    \n",
    "    Returns:\n",
    "        BloomFilter: 생성된 Bloom filter 객체\n",
    "    \"\"\"\n",
    "    # error_rate가 없으면 전역 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    if error_rate is None:\n",
    "        error_rate = globals().get('BLOOM_FILTER_ERROR_RATE', 0.001)\n",
    "    \n",
    "    # error_rate 유효성 검증\n",
    "    if error_rate <= 0 or error_rate >= 1:\n",
    "        raise ValueError(\n",
    "            f\"error_rate는 0과 1 사이의 값이어야 합니다. 현재 값: {error_rate}\"\n",
    "        )\n",
    "    \n",
    "    # 매우 작은 값에 대한 경고 (1e-10보다 작으면)\n",
    "    if error_rate < 1e-10:\n",
    "        import warnings\n",
    "        warnings.warn(\n",
    "            f\"매우 작은 error_rate ({error_rate})를 사용하면 메모리 사용량이 극도로 증가할 수 있습니다. \"\n",
    "            f\"일반적으로 0.001 (0.1%) 이상의 값을 권장합니다.\",\n",
    "            UserWarning\n",
    "        )\n",
    "    \n",
    "    # 고유 n-gram 개수 계산\n",
    "    unique_ngrams = set(ngrams)\n",
    "    capacity = len(unique_ngrams)\n",
    "    \n",
    "    # Bloom filter 생성\n",
    "    bloom = BloomFilter(capacity=capacity, error_rate=error_rate)\n",
    "    \n",
    "    # 모든 고유 n-gram을 Bloom filter에 추가\n",
    "    for ngram in unique_ngrams:\n",
    "        # 튜플을 문자열로 변환하여 추가\n",
    "        bloom.add(str(ngram))\n",
    "    \n",
    "    return bloom\n",
    "\n",
    "def check_ngram_in_bloom(bloom_filter, ngram):\n",
    "    \"\"\"\n",
    "    N-gram이 Bloom filter에 존재하는지 확인하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        ngram: 확인할 n-gram (튜플)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True면 존재 가능 (False positive 가능), False면 확실히 없음\n",
    "    \"\"\"\n",
    "    return str(ngram) in bloom_filter\n",
    "\n",
    "def check_ngrams_batch(bloom_filter, ngrams):\n",
    "    \"\"\"\n",
    "    여러 N-gram을 한 번에 체크하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        ngrams: 확인할 n-gram 리스트\n",
    "    \n",
    "    Returns:\n",
    "        list: 각 n-gram의 존재 여부 (True/False)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for ngram in ngrams:\n",
    "        results.append(check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    return results\n",
    "\n",
    "def analyze_bloom_filter_performance(bloom_filter, known_ngrams, test_ngrams):\n",
    "    \"\"\"\n",
    "    Bloom filter의 성능을 분석하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        known_ngrams: Bloom filter에 추가된 n-gram 리스트 (실제로 존재해야 함)\n",
    "        test_ngrams: 테스트할 n-gram 리스트\n",
    "    \n",
    "    Returns:\n",
    "        dict: 성능 통계\n",
    "    \"\"\"\n",
    "    known_set = set(known_ngrams)\n",
    "    test_set = set(test_ngrams)\n",
    "    \n",
    "    # True Positive: 실제 존재하고 Bloom filter도 True\n",
    "    tp = sum(1 for ngram in test_ngrams if ngram in known_set and check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # False Positive: 실제로 없지만 Bloom filter가 True\n",
    "    fp = sum(1 for ngram in test_ngrams if ngram not in known_set and check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # True Negative: 실제로 없고 Bloom filter도 False\n",
    "    tn = sum(1 for ngram in test_ngrams if ngram not in known_set and not check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # False Negative: 실제로 존재하지만 Bloom filter가 False (이론적으로 발생하지 않아야 함)\n",
    "    fn = sum(1 for ngram in test_ngrams if ngram in known_set and not check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    total = len(test_ngrams)\n",
    "    \n",
    "    stats = {\n",
    "        'total_tested': total,\n",
    "        'true_positive': tp,\n",
    "        'false_positive': fp,\n",
    "        'true_negative': tn,\n",
    "        'false_negative': fn,\n",
    "        'false_positive_rate': fp / total if total > 0 else 0.0,\n",
    "        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0.0,\n",
    "        'recall': tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# N-gram Bloom Filter 생성 및 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"N-gram Bloom Filter 생성 및 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ngrams가 존재하는지 확인\n",
    "if 'ngrams' in globals() and ngrams:\n",
    "    print(f\"\\n1. Bloom Filter 생성 중...\")\n",
    "    print(f\"   입력 N-gram 개수: {len(ngrams)}\")\n",
    "    print(f\"   고유 N-gram 개수: {len(set(ngrams))}\")\n",
    "    \n",
    "    # Bloom filter 생성 (.env에서 설정된 error_rate 사용)\n",
    "    bloom_filter = create_ngram_bloom_filter(ngrams, error_rate=None)  # None이면 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    print(f\"   Bloom Filter 생성 완료!\")\n",
    "    print(f\"   Bloom Filter 크기: {bloom_filter.capacity}개 용량\")\n",
    "    print(f\"   False Positive Rate: {bloom_filter.error_rate * 100:.3f}%\")\n",
    "    \n",
    "    # 샘플 체크\n",
    "    print(f\"\\n2. 샘플 N-gram 체크:\")\n",
    "    print(\"-\" * 80)\n",
    "    sample_ngrams = ngrams[:10]\n",
    "    for i, ngram in enumerate(sample_ngrams, 1):\n",
    "        exists = check_ngram_in_bloom(bloom_filter, ngram)\n",
    "        print(f\"   {i:2d}. {ngram}: {'존재함' if exists else '존재하지 않음'}\")\n",
    "    \n",
    "    # 성능 분석\n",
    "    print(f\"\\n3. Bloom Filter 성능 분석:\")\n",
    "    print(\"-\" * 80)\n",
    "    # 전체 ngrams를 train/test로 분할 (80/20)\n",
    "    split_idx = int(len(ngrams) * 0.8)\n",
    "    train_ngrams = ngrams[:split_idx]\n",
    "    test_ngrams = ngrams[split_idx:]\n",
    "    \n",
    "    # Train set으로 Bloom filter 재생성 (.env에서 설정된 error_rate 사용)\n",
    "    train_bloom = create_ngram_bloom_filter(train_ngrams, error_rate=None)  # None이면 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    \n",
    "    # Test set으로 성능 측정\n",
    "    performance = analyze_bloom_filter_performance(train_bloom, train_ngrams, test_ngrams)\n",
    "    \n",
    "    print(f\"   테스트 N-gram 개수: {performance['total_tested']}\")\n",
    "    print(f\"   True Positive: {performance['true_positive']}\")\n",
    "    print(f\"   False Positive: {performance['false_positive']} ({performance['false_positive_rate']*100:.3f}%)\")\n",
    "    print(f\"   True Negative: {performance['true_negative']}\")\n",
    "    print(f\"   False Negative: {performance['false_negative']}\")\n",
    "    print(f\"   Precision: {performance['precision']*100:.2f}%\")\n",
    "    print(f\"   Recall: {performance['recall']*100:.2f}%\")\n",
    "    \n",
    "    # 새로운 N-gram 체크 예시\n",
    "    print(f\"\\n4. 새로운 N-gram 체크 예시:\")\n",
    "    print(\"-\" * 80)\n",
    "    # 존재하지 않을 가능성이 높은 새로운 n-gram 생성\n",
    "    max_cluster_id = max([max(ng) for ng in ngrams if ng])\n",
    "    new_ngrams = [\n",
    "        tuple([max_cluster_id + 100 + i for i in range(N_GRAM_SIZE)]),  # 존재하지 않을 n-gram\n",
    "        ngrams[0] if ngrams else tuple([0] * N_GRAM_SIZE),  # 존재하는 n-gram\n",
    "    ]\n",
    "    \n",
    "    for ngram in new_ngrams:\n",
    "        exists = check_ngram_in_bloom(bloom_filter, ngram)\n",
    "        print(f\"   {ngram}: {'존재 가능 (False positive 가능)' if exists else '존재하지 않음'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Bloom Filter 생성 완료!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n사용 가능한 변수:\")\n",
    "    print(f\"  - bloom_filter: 생성된 Bloom filter 객체\")\n",
    "    print(f\"  - train_bloom: Train set으로 생성된 Bloom filter\")\n",
    "    print(f\"\\n사용 예시:\")\n",
    "    print(f\"  - check_ngram_in_bloom(bloom_filter, (0, 1))\")\n",
    "    print(f\"  - check_ngrams_batch(bloom_filter, [(0,1), (1,2), (2,3)])\")\n",
    "    \n",
    "else:\n",
    "    print(\"ngrams 변수가 없습니다. 먼저 셀 8을 실행하여 n-gram을 생성해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f53b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine_cluster_mapping 함수는 dataAnalyze.ipynb로 이동되었습니다.\n",
    "# dataAnalyze.ipynb 파일에서 함수를 import하거나 직접 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25019f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d3c0d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
