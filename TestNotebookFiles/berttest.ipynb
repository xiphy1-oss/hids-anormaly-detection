{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca2e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in .\\.venv\\lib\\site-packages (0.8.41)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in .\\.venv\\lib\\site-packages (from hdbscan) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.0 in .\\.venv\\lib\\site-packages (from hdbscan) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in .\\.venv\\lib\\site-packages (from hdbscan) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in .\\.venv\\lib\\site-packages (from hdbscan) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in .\\.venv\\lib\\site-packages (from scikit-learn>=1.6->hdbscan) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 필요 패키지 설치 (처음 1회만 실행)\n",
    "%pip install -q transformers torch\n",
    "%pip install hdbscan\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q pybloom-live\n",
    "# Vector DB 저장을 위한 패키지 설치\n",
    "%pip install -q faiss-cpu\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e86955a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logging' from 'huggingface_hub' (c:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# .env 파일에서 설정 로드\u001b[39;00m\n\u001b[32m      7\u001b[39m load_dotenv()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\__init__.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     33\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     is_pretty_midi_available,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     ClassAttrs,\n\u001b[32m     24\u001b[39m     ClassDocstring,\n\u001b[32m     25\u001b[39m     ImageProcessorArgs,\n\u001b[32m     26\u001b[39m     ModelArgs,\n\u001b[32m     27\u001b[39m     ModelOutputArgs,\n\u001b[32m     28\u001b[39m     auto_class_docstring,\n\u001b[32m     29\u001b[39m     auto_docstring,\n\u001b[32m     30\u001b[39m     get_args_doc_from_source,\n\u001b[32m     31\u001b[39m     parse_docstring,\n\u001b[32m     32\u001b[39m     set_min_indent,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:32\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     28\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     29\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     30\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     35\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m AUTODOC_FILES = [\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:32\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, TypedDict\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_mlx_available, is_torch_available, is_torch_fx_proxy, requires\n\u001b[32m     36\u001b[39m _CAN_RECORD_REGISTRY = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\transformers\\utils\\logging.py:33\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     CRITICAL,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[32m     23\u001b[39m     DEBUG,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     WARNING,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m captureWarnings \u001b[38;5;28;01mas\u001b[39;00m _captureWarnings\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto \u001b[38;5;28;01mas\u001b[39;00m tqdm_lib\n\u001b[32m     37\u001b[39m _lock = threading.Lock()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py:112\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_safetensors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SafetensorsFileMetadata, SafetensorsRepoMetadata, TensorInfo\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_output, run_interactive_subprocess, run_subprocess\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_telemetry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m send_telemetry\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_terminal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ANSI, tabulate\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_jsonable, is_simple_optional_type, unwrap_simple_optional_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_telemetry.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants, logging\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_hf_headers, get_session, hf_raise_for_status\n\u001b[32m     10\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'logging' from 'huggingface_hub' (c:\\project\\hids-anormaly-detection\\.venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# .env 파일에서 설정 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 설정값 가져오기\n",
    "BERT_MODEL_NAME = os.getenv('BERT_MODEL_NAME', 'bert-base-uncased')\n",
    "CLUSTERING_ALPHA = float(os.getenv('CLUSTERING_ALPHA', '1.0'))\n",
    "CLUSTERING_METHOD = os.getenv('CLUSTERING_METHOD', 'dbscan')\n",
    "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', '42'))\n",
    "KEEP_PREFIXES_STR = os.getenv('KEEP_PREFIXES', 'processName,eventName,syscall,args,executable,returnValue')\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '32'))\n",
    "SHUFFLE = os.getenv('SHUFFLE', 'False').lower() == 'true'\n",
    "TOP_N_CLUSTERS = int(os.getenv('TOP_N_CLUSTERS', '10'))\n",
    "N_GRAM_SIZE = int(os.getenv('N_GRAM_SIZE_CONF', '2'))\n",
    "BLOOM_FILTER_ERROR_RATE = float(os.getenv('BLOOM_FILTER_ERROR_RATE', '0.001'))\n",
    "\n",
    "# keep_prefixes를 튜플로 변환\n",
    "keep_prefixes = tuple([prefix.strip() for prefix in KEEP_PREFIXES_STR.split(',')])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"설정 로드 완료\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"BERT 모델: {BERT_MODEL_NAME}\")\n",
    "print(f\"클러스터링 Alpha: {CLUSTERING_ALPHA}\")\n",
    "print(f\"클러스터링 Method: {CLUSTERING_METHOD}\")\n",
    "print(f\"Random State: {CLUSTERING_RANDOM_STATE}\")\n",
    "print(f\"Keep Prefixes: {keep_prefixes}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Shuffle: {SHUFFLE}\")\n",
    "print(f\"Top N Clusters: {TOP_N_CLUSTERS}\")\n",
    "print(f\"N-gram Size: {N_GRAM_SIZE}\")\n",
    "print(f\"Bloom Filter Error Rate: {BLOOM_FILTER_ERROR_RATE} ({BLOOM_FILTER_ERROR_RATE*100:.3f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BERT 다운로드 및 로드\n",
    "extractor = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=BERT_MODEL_NAME,\n",
    "    tokenizer=BERT_MODEL_NAME,\n",
    "    return_tensors=True,\n",
    ")\n",
    "\n",
    "def mean_pool(token_embeddings):\n",
    "    # token_embeddings: (batch, seq_len, hidden_size)\n",
    "    return token_embeddings.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25221a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Literal\n",
    "\n",
    "class TraceeDataset(Dataset):\n",
    "    def __init__(self, data_path, filter_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: JSON Lines 파일 경로\n",
    "            filter_func: 텍스트 필터링 함수 (선택사항)\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.data_path = data_path  # 원본 데이터 경로 저장\n",
    "        self.filter_func = filter_func\n",
    "        self.original_data = []  # 원본 JSON 데이터 저장\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        json_obj = json.loads(line)\n",
    "                        # 원본 데이터 저장\n",
    "                        self.original_data.append(json_obj)\n",
    "                        # JSON 객체를 문자열로 변환\n",
    "                        text = json.dumps(json_obj, ensure_ascii=False)\n",
    "                        if filter_func:\n",
    "                            text = filter_func(text)\n",
    "                        self.data.append(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def getDataLoader(\n",
    "    split: Literal[\"train\", \"validation\"],\n",
    "    data_type: Literal[\"attack\", \"normal\"],\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    filter_func=None,\n",
    "    num_workers: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 로더를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        split: \"train\" 또는 \"validation\"\n",
    "        data_type: \"attack\" 또는 \"normal\"\n",
    "        batch_size: 배치 크기\n",
    "        shuffle: 셔플 여부 (기본값: True)\n",
    "        filter_func: 텍스트 필터링 함수 (선택사항)\n",
    "        num_workers: 데이터 로딩 워커 수 (기본값: 0)\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader 객체\n",
    "    \"\"\"\n",
    "    # 파일명 구성\n",
    "    prefix = \"tr\" if split == \"train\" else \"val\"\n",
    "    filename = f\"{prefix}_{data_type}_tracee.json\"\n",
    "    data_path = os.path.join(\"data\", filename)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"데이터 파일을 찾을 수 없습니다: {data_path}\")\n",
    "    \n",
    "    # Dataset 생성\n",
    "    dataset = TraceeDataset(data_path, filter_func=filter_func)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda x: x  # 배치를 리스트로 반환\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 유사도를 높이는 공통/식별값(IDs, timestamp 등)을 제거하고 핵심 필드만 남깁니다.\n",
    "# keep_prefixes는 .env 파일에서 로드됨 (셀 1에서 설정됨)\n",
    "\n",
    "kv_pattern = re.compile(r'\"([^\"]+)\":\\s*\"([^\"]*)\"')\n",
    "\n",
    "def filter_text(raw_text, keep_keys=None):\n",
    "    \"\"\"\n",
    "    텍스트에서 지정된 키만 남기는 필터링 함수\n",
    "    \n",
    "    Args:\n",
    "        raw_text: 원본 JSON 문자열\n",
    "        keep_keys: 유지할 키의 튜플 (기본값: 전역 keep_prefixes)\n",
    "    \n",
    "    Returns:\n",
    "        필터링된 텍스트 문자열\n",
    "    \"\"\"\n",
    "    if keep_keys is None:\n",
    "        keep_keys = keep_prefixes\n",
    "    pairs = kv_pattern.findall(raw_text)\n",
    "    kept = []\n",
    "    for key, value in pairs:\n",
    "        if key.startswith(keep_keys):\n",
    "            kept.append(f\"{key}={value}\")\n",
    "    return \" \".join(kept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 임베딩이 없습니다. 새로 생성합니다.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m기존 임베딩이 없습니다. 새로 생성합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train normal 데이터 로더 생성 (.env에서 설정값 사용)\u001b[39;00m\n\u001b[32m     25\u001b[39m train_normal_loader = getDataLoader(\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnormal\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     batch_size=\u001b[43mBATCH_SIZE\u001b[49m, \n\u001b[32m     29\u001b[39m     shuffle=SHUFFLE, \n\u001b[32m     30\u001b[39m     filter_func=filter_text\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 모든 데이터를 임베딩으로 변환하여 리스트에 저장\u001b[39;00m\n\u001b[32m     34\u001b[39m embeddings_list = []\n",
      "\u001b[31mNameError\u001b[39m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 설정 로드 (셀 1이 실행되지 않은 경우 대비)\n",
    "if 'BATCH_SIZE' not in globals() or 'SHUFFLE' not in globals():\n",
    "    print(\"설정 변수가 없습니다. .env 파일에서 로드합니다...\")\n",
    "    load_dotenv()\n",
    "    BATCH_SIZE = int(os.getenv('BATCH_SIZE', '32'))\n",
    "    SHUFFLE = os.getenv('SHUFFLE', 'False').lower() == 'true'\n",
    "    print(f\"BATCH_SIZE: {BATCH_SIZE}, SHUFFLE: {SHUFFLE}\")\n",
    "\n",
    "# 기존 임베딩이 있는지 확인\n",
    "embeddings_file = \"./embeddings_cache/train_normal_embeddings.pkl\"\n",
    "\n",
    "# 1. 메모리에 이미 embeddings_tensor가 있는지 확인\n",
    "if 'embeddings_tensor' in globals() and globals()['embeddings_tensor'] is not None:\n",
    "    embeddings_tensor = globals()['embeddings_tensor']\n",
    "    print(\"기존 임베딩을 메모리에서 발견했습니다.\")\n",
    "    print(f\"임베딩 텐서 shape: {embeddings_tensor.shape}\")\n",
    "elif os.path.exists(embeddings_file):\n",
    "    # 2. 파일로 저장된 임베딩이 있는지 확인\n",
    "    print(f\"기존 임베딩 파일을 발견했습니다: {embeddings_file}\")\n",
    "    print(\"임베딩 로드 중...\")\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        embeddings_tensor = pickle.load(f)\n",
    "    print(f\"임베딩 로드 완료! 임베딩 텐서 shape: {embeddings_tensor.shape}\")\n",
    "else:\n",
    "    # 3. 새로 임베딩 생성\n",
    "    print(\"기존 임베딩이 없습니다. 새로 생성합니다.\")\n",
    "    \n",
    "    # filter_text 함수가 없으면 정의\n",
    "    if 'filter_text' not in globals():\n",
    "        import re\n",
    "        if 'keep_prefixes' not in globals():\n",
    "            KEEP_PREFIXES_STR = os.getenv('KEEP_PREFIXES', 'processName,eventName,syscall,args,executable,returnValue')\n",
    "            keep_prefixes = tuple([prefix.strip() for prefix in KEEP_PREFIXES_STR.split(',')])\n",
    "        \n",
    "        kv_pattern = re.compile(r'\"([^\"]+)\":\\s*\"([^\"]*)\"')\n",
    "        def filter_text(raw_text, keep_keys=None):\n",
    "            if keep_keys is None:\n",
    "                keep_keys = keep_prefixes\n",
    "            pairs = kv_pattern.findall(raw_text)\n",
    "            kept = []\n",
    "            for key, value in pairs:\n",
    "                if key.startswith(keep_keys):\n",
    "                    kept.append(f\"{key}={value}\")\n",
    "            return \" \".join(kept)\n",
    "    \n",
    "    # extractor와 mean_pool 함수가 없으면 로드\n",
    "    if 'extractor' not in globals() or 'mean_pool' not in globals():\n",
    "        print(\"BERT 모델과 함수를 로드합니다...\")\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        BERT_MODEL_NAME = os.getenv('BERT_MODEL_NAME', 'bert-base-uncased')\n",
    "        extractor = pipeline(\n",
    "            \"feature-extraction\",\n",
    "            model=BERT_MODEL_NAME,\n",
    "            tokenizer=BERT_MODEL_NAME,\n",
    "            return_tensors=True,\n",
    "        )\n",
    "        \n",
    "        def mean_pool(token_embeddings):\n",
    "            # token_embeddings: (batch, seq_len, hidden_size)\n",
    "            return token_embeddings.mean(dim=1)\n",
    "    \n",
    "    # Train normal 데이터 로더 생성 (.env에서 설정값 사용)\n",
    "    train_normal_loader = getDataLoader(\n",
    "        \"train\", \n",
    "        \"normal\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=SHUFFLE, \n",
    "        filter_func=filter_text\n",
    "    )\n",
    "    \n",
    "    # 모든 데이터를 임베딩으로 변환하여 리스트에 저장\n",
    "    embeddings_list = []\n",
    "    \n",
    "    print(f\"총 데이터 개수: {len(train_normal_loader.dataset)}\")\n",
    "    print(\"임베딩 생성 중...\")\n",
    "    \n",
    "    for batch_idx, batch_texts in enumerate(tqdm(train_normal_loader, desc=\"임베딩 생성\")):\n",
    "        # 배치 내 각 텍스트에 대해 임베딩 생성\n",
    "        for text in batch_texts:\n",
    "            # 문장 그대로 입력하면 내부에서 토크나이즈가 처리됨\n",
    "            outputs = extractor(text, padding=True, truncation=True)  # Shape: (1, seq_len, hidden_size)\n",
    "            # 평균 풀링하여 문장 임베딩 생성\n",
    "            pooled_embedding = mean_pool(outputs)  # Shape: (1, hidden_size)\n",
    "            # 배치 차원 제거하고 리스트에 추가\n",
    "            embeddings_list.append(pooled_embedding.squeeze(0))  # Shape: (hidden_size,)\n",
    "    \n",
    "    # 리스트를 텐서로 변환\n",
    "    import torch\n",
    "    embeddings_tensor = torch.stack(embeddings_list)  # Shape: (num_samples, hidden_size)\n",
    "    \n",
    "    # 생성된 임베딩을 파일로 저장\n",
    "    os.makedirs(\"./embeddings_cache\", exist_ok=True)\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        pickle.dump(embeddings_tensor, f)\n",
    "    print(f\"임베딩이 파일로 저장되었습니다: {embeddings_file}\")\n",
    "    \n",
    "    print(f\"\\n완료! 임베딩 리스트 길이: {len(embeddings_list)}\")\n",
    "    print(f\"임베딩 텐서 shape: {embeddings_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c10f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingVectorDB 클래스가 정의되었습니다. (FAISS 사용)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class EmbeddingVectorDB:\n",
    "    \"\"\"\n",
    "    FAISS를 사용하여 임베딩을 Vector DB에 저장하고 검색하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name: str = \"hids_embeddings\", persist_directory: str = \"./faiss_db\", index_type: str = \"flat\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            collection_name: 컬렉션 이름\n",
    "            persist_directory: 데이터베이스 저장 디렉토리\n",
    "            index_type: 인덱스 타입 (\"flat\" 또는 \"ivf\")\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.index_type = index_type\n",
    "        self.index = None\n",
    "        self.metadata = []\n",
    "        self.ids = []\n",
    "        self.embedding_dim = None\n",
    "        \n",
    "        # 저장 디렉토리 생성\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        \n",
    "        # 파일 경로 설정\n",
    "        self.index_path = os.path.join(persist_directory, f\"{collection_name}.index\")\n",
    "        self.metadata_path = os.path.join(persist_directory, f\"{collection_name}_metadata.json\")\n",
    "        self.ids_path = os.path.join(persist_directory, f\"{collection_name}_ids.json\")\n",
    "        \n",
    "        # 기존 인덱스가 있으면 로드\n",
    "        if os.path.exists(self.index_path):\n",
    "            self._load_index()\n",
    "            print(f\"기존 인덱스 '{collection_name}'을 불러왔습니다. (임베딩 수: {self.index.ntotal})\")\n",
    "        else:\n",
    "            print(f\"새 인덱스 '{collection_name}'을 생성합니다.\")\n",
    "    \n",
    "    def _load_index(self):\n",
    "        \"\"\"기존 인덱스 로드\"\"\"\n",
    "        self.index = faiss.read_index(self.index_path)\n",
    "        self.embedding_dim = self.index.d\n",
    "        \n",
    "        # 메타데이터와 ID 로드\n",
    "        if os.path.exists(self.metadata_path):\n",
    "            with open(self.metadata_path, 'r', encoding='utf-8') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        if os.path.exists(self.ids_path):\n",
    "            with open(self.ids_path, 'r', encoding='utf-8') as f:\n",
    "                self.ids = json.load(f)\n",
    "    \n",
    "    def _create_index(self, embedding_dim: int):\n",
    "        \"\"\"FAISS 인덱스 생성\"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        if self.index_type == \"flat\":\n",
    "            # 완전 검색 인덱스 (정확하지만 느림)\n",
    "            self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        elif self.index_type == \"ivf\":\n",
    "            # IVF 인덱스 (빠르지만 근사 검색)\n",
    "            nlist = 100  # 클러스터 수\n",
    "            quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 인덱스 타입: {self.index_type}\")\n",
    "    \n",
    "    def save_embeddings(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        metadata_list: List[Dict],\n",
    "        ids: Optional[List[str]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        임베딩을 Vector DB에 저장\n",
    "        \n",
    "        Args:\n",
    "            embeddings: 임베딩 배열 (n_samples, embedding_dim)\n",
    "            metadata_list: 각 임베딩에 대한 메타데이터 리스트\n",
    "            ids: 각 임베딩의 고유 ID 리스트 (None이면 자동 생성)\n",
    "        \"\"\"\n",
    "        # 텐서를 numpy 배열로 변환\n",
    "        if isinstance(embeddings, torch.Tensor):\n",
    "            embeddings_np = embeddings.detach().cpu().numpy().astype('float32')\n",
    "        else:\n",
    "            embeddings_np = np.array(embeddings, dtype='float32')\n",
    "        \n",
    "        n_samples, embedding_dim = embeddings_np.shape\n",
    "        \n",
    "        # ID가 없으면 자동 생성\n",
    "        if ids is None:\n",
    "            ids = [f\"embedding_{i}\" for i in range(n_samples)]\n",
    "        \n",
    "        # 인덱스가 없으면 생성\n",
    "        if self.index is None:\n",
    "            self._create_index(embedding_dim)\n",
    "        elif self.index.d != embedding_dim:\n",
    "            raise ValueError(f\"임베딩 차원이 일치하지 않습니다. 기존: {self.index.d}, 새로운: {embedding_dim}\")\n",
    "        \n",
    "        # IVF 인덱스인 경우 훈련 필요\n",
    "        if self.index_type == \"ivf\" and not self.index.is_trained:\n",
    "            print(\"IVF 인덱스 훈련 중...\")\n",
    "            self.index.train(embeddings_np)\n",
    "        \n",
    "        # 임베딩 추가\n",
    "        self.index.add(embeddings_np)\n",
    "        \n",
    "        # 메타데이터와 ID 저장\n",
    "        processed_metadata = []\n",
    "        for meta in metadata_list:\n",
    "            processed_meta = {}\n",
    "            for key, value in meta.items():\n",
    "                # JSON 직렬화 가능한 타입만 저장\n",
    "                if isinstance(value, (str, int, float, bool, type(None))):\n",
    "                    processed_meta[key] = value\n",
    "                else:\n",
    "                    # 복잡한 객체는 JSON 문자열로 변환\n",
    "                    processed_meta[key] = json.dumps(value, ensure_ascii=False)\n",
    "            processed_metadata.append(processed_meta)\n",
    "        \n",
    "        self.metadata.extend(processed_metadata)\n",
    "        self.ids.extend(ids)\n",
    "        \n",
    "        # 인덱스와 메타데이터 저장\n",
    "        self._save_index()\n",
    "        \n",
    "        print(f\"총 {n_samples}개의 임베딩이 저장되었습니다. (전체: {self.index.ntotal}개)\")\n",
    "    \n",
    "    def _save_index(self):\n",
    "        \"\"\"인덱스와 메타데이터를 파일로 저장\"\"\"\n",
    "        faiss.write_index(self.index, self.index_path)\n",
    "        with open(self.metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\n",
    "        with open(self.ids_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.ids, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def search_similar(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        n_results: int = 10,\n",
    "        where: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        유사한 임베딩 검색\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: 검색할 임베딩 벡터\n",
    "            n_results: 반환할 결과 개수\n",
    "            where: 메타데이터 필터 조건 (FAISS는 기본적으로 필터링 미지원, 후처리로 처리)\n",
    "        \n",
    "        Returns:\n",
    "            검색 결과 딕셔너리\n",
    "        \"\"\"\n",
    "        if self.index is None or self.index.ntotal == 0:\n",
    "            raise ValueError(\"인덱스가 비어있습니다. 먼저 임베딩을 저장해주세요.\")\n",
    "        \n",
    "        # 텐서를 numpy 배열로 변환\n",
    "        if isinstance(query_embedding, torch.Tensor):\n",
    "            query_embedding = query_embedding.detach().cpu().numpy().astype('float32')\n",
    "        else:\n",
    "            query_embedding = np.array(query_embedding, dtype='float32')\n",
    "        \n",
    "        # 단일 벡터인 경우 2D로 변환\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # 검색 (더 많은 결과를 가져와서 필터링 가능하도록)\n",
    "        k = min(n_results * 3 if where else n_results, self.index.ntotal)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # 결과 처리\n",
    "        result_ids = []\n",
    "        result_distances = []\n",
    "        result_metadatas = []\n",
    "        \n",
    "        for i, (dist_row, idx_row) in enumerate(zip(distances, indices)):\n",
    "            ids_row = []\n",
    "            dists_row = []\n",
    "            metadatas_row = []\n",
    "            \n",
    "            for dist, idx in zip(dist_row, idx_row):\n",
    "                if idx == -1:  # FAISS에서 유효하지 않은 인덱스\n",
    "                    continue\n",
    "                \n",
    "                # 메타데이터 필터링\n",
    "                if where:\n",
    "                    metadata = self.metadata[idx]\n",
    "                    match = True\n",
    "                    for key, value in where.items():\n",
    "                        if metadata.get(key) != value:\n",
    "                            match = False\n",
    "                            break\n",
    "                    if not match:\n",
    "                        continue\n",
    "                \n",
    "                ids_row.append(self.ids[idx])\n",
    "                dists_row.append(float(dist))\n",
    "                metadatas_row.append(self.metadata[idx])\n",
    "                \n",
    "                if len(ids_row) >= n_results:\n",
    "                    break\n",
    "            \n",
    "            result_ids.append(ids_row)\n",
    "            result_distances.append(dists_row)\n",
    "            result_metadatas.append(metadatas_row)\n",
    "        \n",
    "        return {\n",
    "            'ids': result_ids,\n",
    "            'distances': result_distances,\n",
    "            'metadatas': result_metadatas\n",
    "        }\n",
    "    \n",
    "    def get_all_embeddings(self) -> Dict:\n",
    "        \"\"\"\n",
    "        모든 임베딩 가져오기 (FAISS는 벡터만 저장하므로 메타데이터만 반환)\n",
    "        \n",
    "        Returns:\n",
    "            모든 메타데이터와 ID\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'ids': self.ids,\n",
    "            'metadatas': self.metadata,\n",
    "            'total': len(self.ids)\n",
    "        }\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        \"\"\"컬렉션 삭제\"\"\"\n",
    "        if os.path.exists(self.index_path):\n",
    "            os.remove(self.index_path)\n",
    "        if os.path.exists(self.metadata_path):\n",
    "            os.remove(self.metadata_path)\n",
    "        if os.path.exists(self.ids_path):\n",
    "            os.remove(self.ids_path)\n",
    "        \n",
    "        self.index = None\n",
    "        self.metadata = []\n",
    "        self.ids = []\n",
    "        \n",
    "        print(f\"컬렉션 '{self.collection_name}'이 삭제되었습니다.\")\n",
    "    \n",
    "    def get_collection_info(self) -> Dict:\n",
    "        \"\"\"컬렉션 정보 반환\"\"\"\n",
    "        total = self.index.ntotal if self.index else 0\n",
    "        return {\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"total_embeddings\": total,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"index_type\": self.index_type,\n",
    "            \"persist_directory\": self.persist_directory\n",
    "        }\n",
    "\n",
    "print(\"EmbeddingVectorDB 클래스가 정의되었습니다. (FAISS 사용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d7796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_tensor 또는 train_normal_loader가 아직 생성되지 않았습니다.\n",
      "먼저 셀 4를 실행해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 임베딩을 Vector DB에 저장\n",
    "if 'embeddings_tensor' in globals() and 'train_normal_loader' in globals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"임베딩을 Vector DB에 저장 중...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Vector DB 인스턴스 생성\n",
    "    vector_db = EmbeddingVectorDB(\n",
    "        collection_name=\"hids_normal_embeddings\",\n",
    "        persist_directory=\"./faiss_db\",\n",
    "        index_type=\"flat\"  # \"flat\" (완전 검색) 또는 \"ivf\" (빠른 근사 검색)\n",
    "    )\n",
    "    \n",
    "    # 메타데이터 준비\n",
    "    dataset = train_normal_loader.dataset\n",
    "    metadata_list = []\n",
    "    \n",
    "    print(\"메타데이터 준비 중...\")\n",
    "    for idx in range(len(dataset)):\n",
    "        # 원본 데이터 가져오기\n",
    "        if hasattr(dataset, 'original_data') and idx < len(dataset.original_data):\n",
    "            original_data = dataset.original_data[idx]\n",
    "        else:\n",
    "            original_data = {}\n",
    "        \n",
    "        # 필터링된 텍스트 가져오기\n",
    "        filtered_text = dataset[idx]\n",
    "        \n",
    "        # 메타데이터 생성\n",
    "        metadata = {\n",
    "            \"index\": idx,\n",
    "            \"text\": filtered_text,\n",
    "            \"processName\": original_data.get(\"processName\", \"N/A\"),\n",
    "            \"eventName\": original_data.get(\"eventName\", \"N/A\"),\n",
    "            \"syscall\": original_data.get(\"syscall\", \"N/A\"),\n",
    "            \"eventId\": str(original_data.get(\"eventId\", \"N/A\")),\n",
    "        }\n",
    "        \n",
    "        # 클러스터 ID가 있으면 추가\n",
    "        if 'labels' in globals() and idx < len(labels):\n",
    "            metadata[\"cluster_id\"] = int(labels[idx])\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # ID 생성\n",
    "    ids = [f\"normal_{i}\" for i in range(len(embeddings_tensor))]\n",
    "    \n",
    "    # Vector DB에 저장\n",
    "    print(\"Vector DB에 저장 중...\")\n",
    "    vector_db.save_embeddings(\n",
    "        embeddings=embeddings_tensor,\n",
    "        metadata_list=metadata_list,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    # 컬렉션 정보 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"컬렉션 정보\")\n",
    "    print(\"=\" * 80)\n",
    "    info = vector_db.get_collection_info()\n",
    "    for key, value in info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"저장 완료!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n사용 가능한 메서드:\")\n",
    "    print(f\"  - vector_db.search_similar(query_embedding, n_results=10)\")\n",
    "    print(f\"  - vector_db.get_all_embeddings()\")\n",
    "    print(f\"  - vector_db.get_collection_info()\")\n",
    "    \n",
    "else:\n",
    "    print(\"embeddings_tensor 또는 train_normal_loader가 아직 생성되지 않았습니다.\")\n",
    "    print(\"먼저 셀 4를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_db가 아직 생성되지 않았습니다.\n",
      "먼저 셀 7을 실행해주세요.\n"
     ]
    }
   ],
   "source": [
    "# Vector DB 검색 테스트\n",
    "if 'vector_db' in globals() and 'embeddings_tensor' in globals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Vector DB 검색 테스트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 첫 번째 임베딩으로 유사도 검색\n",
    "    query_embedding = embeddings_tensor[0]\n",
    "    \n",
    "    print(f\"\\n1. 첫 번째 임베딩과 유사한 상위 5개 검색:\")\n",
    "    print(\"-\" * 80)\n",
    "    results = vector_db.search_similar(query_embedding, n_results=5)\n",
    "    \n",
    "    if results['ids'][0]:  # 결과가 있는지 확인\n",
    "        for i, (id_val, distance, metadata) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['distances'][0],\n",
    "            results['metadatas'][0]\n",
    "        ), 1):\n",
    "            print(f\"\\n{i}. ID: {id_val}\")\n",
    "            print(f\"   거리: {distance:.4f}\")\n",
    "            print(f\"   프로세스: {metadata.get('processName', 'N/A')}\")\n",
    "            print(f\"   이벤트: {metadata.get('eventName', 'N/A')}\")\n",
    "            print(f\"   시스템 콜: {metadata.get('syscall', 'N/A')}\")\n",
    "            if 'cluster_id' in metadata:\n",
    "                print(f\"   클러스터 ID: {metadata['cluster_id']}\")\n",
    "    else:\n",
    "        print(\"   검색 결과가 없습니다.\")\n",
    "    \n",
    "    # 특정 클러스터의 임베딩 검색 (메타데이터 필터 사용)\n",
    "    if 'labels' in globals():\n",
    "        print(f\"\\n2. 클러스터 0에 속한 임베딩 검색:\")\n",
    "        print(\"-\" * 80)\n",
    "        cluster_0_indices = np.where(labels == 0)[0]\n",
    "        if len(cluster_0_indices) > 0:\n",
    "            query_embedding_cluster = embeddings_tensor[cluster_0_indices[0]]\n",
    "            results_cluster = vector_db.search_similar(\n",
    "                query_embedding_cluster,\n",
    "                n_results=5,\n",
    "                where={\"cluster_id\": 0}  # 클러스터 0만 검색\n",
    "            )\n",
    "            \n",
    "            if results_cluster['ids'][0]:  # 결과가 있는지 확인\n",
    "                for i, (id_val, distance, metadata) in enumerate(zip(\n",
    "                    results_cluster['ids'][0],\n",
    "                    results_cluster['distances'][0],\n",
    "                    results_cluster['metadatas'][0]\n",
    "                ), 1):\n",
    "                    print(f\"\\n{i}. ID: {id_val}\")\n",
    "                    print(f\"   거리: {distance:.4f}\")\n",
    "                    print(f\"   프로세스: {metadata.get('processName', 'N/A')}\")\n",
    "                    print(f\"   이벤트: {metadata.get('eventName', 'N/A')}\")\n",
    "            else:\n",
    "                print(\"   검색 결과가 없습니다.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"검색 테스트 완료!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"vector_db가 아직 생성되지 않았습니다.\")\n",
    "    print(\"먼저 셀 7을 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a124dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# HDBSCAN은 별도 설치 필요: pip install hdbscan\n",
    "try:\n",
    "    import hdbscan\n",
    "    HDBSCAN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HDBSCAN_AVAILABLE = False\n",
    "    print(\"HDBSCAN을 사용하려면 'pip install hdbscan'을 실행하세요.\")\n",
    "\n",
    "def cluster_embeddings(embeddings, alpha=1.0, method='kmeans', random_state=42):\n",
    "    \"\"\"\n",
    "    임베딩을 클러스터링하는 함수\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 텐서 또는 numpy 배열 (n_samples, embedding_dim)\n",
    "        alpha: 클러스터링 비율 조절 파라미터 (0.1 ~ 2.0)\n",
    "              - alpha가 낮을수록 적은 클러스터 (큰 그룹)\n",
    "              - alpha가 높을수록 많은 클러스터 (세밀한 그룹)\n",
    "        method: 클러스터링 방법 ('kmeans' 또는 'dbscan')\n",
    "        random_state: 랜덤 시드\n",
    "    \n",
    "    Returns:\n",
    "        labels: 각 샘플의 클러스터 레이블 (numpy array)\n",
    "        n_clusters: 생성된 클러스터 수\n",
    "        cluster_info: 클러스터 정보 딕셔너리\n",
    "    \"\"\"\n",
    "    # 텐서를 numpy 배열로 변환\n",
    "    if isinstance(embeddings, torch.Tensor):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = np.array(embeddings)\n",
    "    \n",
    "    n_samples = embeddings_np.shape[0]\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        # alpha 값에 따라 클러스터 수 결정\n",
    "        # alpha=0.1 -> sqrt(n_samples/10), alpha=1.0 -> sqrt(n_samples), alpha=2.0 -> sqrt(n_samples*10)\n",
    "        base_clusters = int(np.sqrt(n_samples))\n",
    "        n_clusters = max(2, int(base_clusters * alpha))\n",
    "        \n",
    "        # K-means 클러스터링\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 실루엣 점수 계산\n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'kmeans',\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'inertia': kmeans.inertia_,\n",
    "            'centers': kmeans.cluster_centers_\n",
    "        }\n",
    "        \n",
    "    elif method == 'dbscan':\n",
    "        # alpha 값에 따라 eps와 min_samples 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 eps), 높을수록 작은 클러스터 (작은 eps)\n",
    "        # 기본 eps는 데이터의 평균 거리의 일정 비율\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        neighbors = NearestNeighbors(n_neighbors=min(10, n_samples))\n",
    "        neighbors_fit = neighbors.fit(embeddings_np)\n",
    "        distances, indices = neighbors_fit.kneighbors(embeddings_np)\n",
    "        distances = np.sort(distances, axis=0)\n",
    "        distances = distances[:, 1]\n",
    "        \n",
    "        # 기본 eps는 거리의 중앙값\n",
    "        base_eps = np.median(distances)\n",
    "        # alpha에 따라 조절: alpha=0.1 -> 큰 eps, alpha=2.0 -> 작은 eps\n",
    "        eps = base_eps / alpha\n",
    "        \n",
    "        min_samples = max(3, int(np.log(n_samples) * alpha))\n",
    "        \n",
    "        # DBSCAN 클러스터링\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산 (노이즈 제외)\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # 실루엣 점수 계산 (노이즈 제외)\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np[mask], labels[mask])\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'dbscan',\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples\n",
    "        }\n",
    "    \n",
    "    elif method == 'hdbscan':\n",
    "        # HDBSCAN: DBSCAN의 개선 버전, 클러스터 개수 자동 결정\n",
    "        # 밀도 기반 계층적 클러스터링, 다양한 크기의 클러스터 처리에 우수\n",
    "        if not HDBSCAN_AVAILABLE:\n",
    "            raise ImportError(\"HDBSCAN이 설치되지 않았습니다. 'pip install hdbscan'을 실행하세요.\")\n",
    "        \n",
    "        # alpha 값에 따라 min_cluster_size와 min_samples 조절\n",
    "        # alpha가 낮을수록 큰 클러스터, 높을수록 작은 클러스터\n",
    "        min_cluster_size = max(2, int(n_samples / (10 * alpha)))\n",
    "        min_samples = max(3, int(np.log(n_samples) * alpha))\n",
    "        \n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=0.0\n",
    "        )\n",
    "        labels = clusterer.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산 (노이즈 제외)\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # 실루엣 점수 계산 (노이즈 제외)\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np[mask], labels[mask])\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'hdbscan',\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'min_samples': min_samples,\n",
    "            'cluster_probabilities': clusterer.probabilities_\n",
    "        }\n",
    "    \n",
    "    elif method == 'agglomerative':\n",
    "        # Agglomerative Clustering: 거리 threshold로 클러스터 개수 자동 결정\n",
    "        # 계층적 클러스터링, 거리 기반으로 클러스터 수 자동 결정\n",
    "        \n",
    "        # 거리 행렬 계산\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        distances = pairwise_distances(embeddings_np)\n",
    "        \n",
    "        # alpha 값에 따라 거리 threshold 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 threshold), 높을수록 작은 클러스터 (작은 threshold)\n",
    "        base_threshold = np.percentile(distances[distances > 0], 50)  # 중앙값\n",
    "        distance_threshold = base_threshold / alpha\n",
    "        \n",
    "        # Agglomerative Clustering\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=distance_threshold,\n",
    "            linkage='ward'\n",
    "        )\n",
    "        labels = clustering.fit_predict(embeddings_np)\n",
    "        \n",
    "        # 클러스터 수 계산\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels)\n",
    "        \n",
    "        # 실루엣 점수 계산\n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "        else:\n",
    "            silhouette_avg = 0.0\n",
    "        \n",
    "        cluster_info = {\n",
    "            'method': 'agglomerative',\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'distance_threshold': distance_threshold\n",
    "        }\n",
    "    \n",
    "    elif method == 'meanshift':\n",
    "        # Mean Shift: 클러스터 개수 자동 결정, 밀도 기반\n",
    "        # bandwidth(대역폭)에 따라 클러스터 수 결정\n",
    "        \n",
    "        # alpha 값에 따라 bandwidth 조절\n",
    "        # alpha가 낮을수록 큰 클러스터 (큰 bandwidth), 높을수록 작은 클러스터 (작은 bandwidth)\n",
    "        from sklearn.cluster import estimate_bandwidth\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        # 거리 기반으로 bandwidth 추정 (더 안정적)\n",
    "        try:\n",
    "            # 샘플링하여 거리 계산 (대용량 데이터 처리)\n",
    "            sample_size = min(500, n_samples)\n",
    "            if n_samples > sample_size:\n",
    "                sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "                sample_data = embeddings_np[sample_indices]\n",
    "            else:\n",
    "                sample_data = embeddings_np\n",
    "            \n",
    "            # 거리 행렬 계산\n",
    "            distances = pairwise_distances(sample_data)\n",
    "            # 0이 아닌 거리의 중앙값 사용\n",
    "            non_zero_distances = distances[distances > 0]\n",
    "            if len(non_zero_distances) > 0:\n",
    "                base_bandwidth = np.median(non_zero_distances)\n",
    "            else:\n",
    "                # fallback: estimate_bandwidth 사용\n",
    "                base_bandwidth = estimate_bandwidth(embeddings_np, quantile=0.3, n_samples=sample_size)\n",
    "        except:\n",
    "            # fallback: estimate_bandwidth 사용\n",
    "            base_bandwidth = estimate_bandwidth(embeddings_np, quantile=0.3, n_samples=min(500, n_samples))\n",
    "        \n",
    "        # alpha에 따라 조절: alpha가 낮을수록 큰 bandwidth\n",
    "        # 최소 bandwidth 보장 (너무 작으면 에러 발생)\n",
    "        bandwidth = max(base_bandwidth / alpha, base_bandwidth * 0.1)\n",
    "        \n",
    "        # Mean Shift 클러스터링 (에러 처리 포함)\n",
    "        try:\n",
    "            ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, max_iter=300)\n",
    "            labels = ms.fit_predict(embeddings_np)\n",
    "            \n",
    "            # 클러스터 수 계산\n",
    "            unique_labels = set(labels)\n",
    "            n_clusters = len(unique_labels)\n",
    "            \n",
    "            # 실루엣 점수 계산\n",
    "            if n_clusters > 1:\n",
    "                silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "            else:\n",
    "                silhouette_avg = 0.0\n",
    "            \n",
    "            cluster_info = {\n",
    "                'method': 'meanshift',\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': silhouette_avg,\n",
    "                'bandwidth': bandwidth\n",
    "            }\n",
    "        except ValueError as e:\n",
    "            # bandwidth가 너무 작아서 에러 발생 시, 자동으로 증가시켜 재시도\n",
    "            if \"bandwidth\" in str(e).lower():\n",
    "                # bandwidth를 2배로 증가\n",
    "                bandwidth = bandwidth * 2\n",
    "                try:\n",
    "                    ms = MeanShift(bandwidth=bandwidth, bin_seeding=False, max_iter=300)\n",
    "                    labels = ms.fit_predict(embeddings_np)\n",
    "                    unique_labels = set(labels)\n",
    "                    n_clusters = len(unique_labels)\n",
    "                    if n_clusters > 1:\n",
    "                        silhouette_avg = silhouette_score(embeddings_np, labels)\n",
    "                    else:\n",
    "                        silhouette_avg = 0.0\n",
    "                    cluster_info = {\n",
    "                        'method': 'meanshift',\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'silhouette_score': silhouette_avg,\n",
    "                        'bandwidth': bandwidth,\n",
    "                        'note': 'bandwidth 자동 조정됨'\n",
    "                    }\n",
    "                except:\n",
    "                    # 여전히 실패하면 모든 포인트를 하나의 클러스터로\n",
    "                    labels = np.zeros(n_samples, dtype=int)\n",
    "                    cluster_info = {\n",
    "                        'method': 'meanshift',\n",
    "                        'n_clusters': 1,\n",
    "                        'silhouette_score': 0.0,\n",
    "                        'bandwidth': bandwidth,\n",
    "                        'note': 'bandwidth 조정 실패, 모든 포인트를 하나의 클러스터로 처리'\n",
    "                    }\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 클러스터링 방법: {method}. \"\n",
    "                        f\"사용 가능한 방법: 'kmeans', 'dbscan', 'hdbscan', 'agglomerative', 'meanshift'\")\n",
    "    \n",
    "    return labels, n_clusters, cluster_info\n",
    "\n",
    "# 사용 예시 및 알고리즘 비교\n",
    "if 'embeddings_tensor' in globals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"클러스터 개수를 미리 지정하지 않는 알고리즘들 비교\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # DBSCAN 테스트\n",
    "    print(\"\\n[DBSCAN] - 밀도 기반, 노이즈 포인트 식별 가능\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='dbscan'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 노이즈: {info['n_noise']}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # HDBSCAN 테스트 (가장 추천!)\n",
    "    if HDBSCAN_AVAILABLE:\n",
    "        print(\"\\n[HDBSCAN] - DBSCAN 개선 버전, 다양한 크기 클러스터 처리 우수 (추천!)\")\n",
    "        print(\"-\" * 80)\n",
    "        for alpha in [0.5, 1.0, 1.5]:\n",
    "            try:\n",
    "                labels, n_clusters, info = cluster_embeddings(\n",
    "                    embeddings_tensor, \n",
    "                    alpha=alpha, \n",
    "                    method='hdbscan'\n",
    "                )\n",
    "                print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 노이즈: {info['n_noise']}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Alpha: {alpha:.1f} -> 오류: {e}\")\n",
    "    else:\n",
    "        print(\"\\n[HDBSCAN] - 설치 필요: pip install hdbscan\")\n",
    "    \n",
    "    # Agglomerative Clustering 테스트\n",
    "    print(\"\\n[Agglomerative Clustering] - 계층적, 거리 threshold 기반\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='agglomerative'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # Mean Shift 테스트\n",
    "    print(\"\\n[Mean Shift] - 밀도 기반, bandwidth 조절\")\n",
    "    print(\"-\" * 80)\n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        labels, n_clusters, info = cluster_embeddings(\n",
    "            embeddings_tensor, \n",
    "            alpha=alpha, \n",
    "            method='meanshift'\n",
    "        )\n",
    "        print(f\"Alpha: {alpha:.1f} -> 클러스터 수: {n_clusters}, 실루엣: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"추천: 클러스터 개수가 많고 미리 지정할 수 없다면 HDBSCAN 또는 DBSCAN 사용\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"embeddings_tensor가 아직 생성되지 않았습니다. 먼저 셀 4를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f92399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "class ClusterTracer:\n",
    "    \"\"\"\n",
    "    클러스터링 결과와 원본 데이터를 매칭하여 추적하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, cluster_labels, filter_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataloader: 데이터 로더 (원본 데이터 접근용)\n",
    "            cluster_labels: 클러스터링 결과 레이블 (numpy array)\n",
    "            filter_func: 필터링 함수 (원본 데이터 로드 시 사용)\n",
    "        \"\"\"\n",
    "        self.dataloader = dataloader\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.filter_func = filter_func\n",
    "        self.original_data = []\n",
    "        self.cluster_to_indices = defaultdict(list)\n",
    "        \n",
    "        # 원본 데이터 로드\n",
    "        self._load_original_data()\n",
    "        \n",
    "        # 클러스터별 인덱스 매핑\n",
    "        self._map_clusters()\n",
    "    \n",
    "    def _load_original_data(self):\n",
    "        \"\"\"원본 JSON 데이터를 로드\"\"\"\n",
    "        dataset = self.dataloader.dataset\n",
    "        \n",
    "        # TraceeDataset에서 원본 데이터 가져오기\n",
    "        if hasattr(dataset, 'original_data'):\n",
    "            self.original_data = dataset.original_data\n",
    "        elif hasattr(dataset, 'data_path'):\n",
    "            # 원본 파일에서 다시 로드\n",
    "            data_path = dataset.data_path\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            json_obj = json.loads(line)\n",
    "                            self.original_data.append(json_obj)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        else:\n",
    "            # 데이터 로더에서 직접 가져오기 (필터링된 버전)\n",
    "            for i in range(len(dataset)):\n",
    "                text = dataset[i]\n",
    "                # 필터링된 텍스트를 저장\n",
    "                self.original_data.append({'filtered_text': text})\n",
    "    \n",
    "    def _map_clusters(self):\n",
    "        \"\"\"클러스터 레이블과 데이터 인덱스를 매핑\"\"\"\n",
    "        for idx, label in enumerate(self.cluster_labels):\n",
    "            self.cluster_to_indices[label].append(idx)\n",
    "    \n",
    "    def get_cluster_data(self, cluster_id):\n",
    "        \"\"\"\n",
    "        특정 클러스터에 속한 모든 데이터 반환\n",
    "        \n",
    "        Args:\n",
    "            cluster_id: 클러스터 ID\n",
    "        \n",
    "        Returns:\n",
    "            해당 클러스터에 속한 데이터 리스트\n",
    "        \"\"\"\n",
    "        if cluster_id not in self.cluster_to_indices:\n",
    "            return []\n",
    "        \n",
    "        indices = self.cluster_to_indices[cluster_id]\n",
    "        return [self.original_data[idx] for idx in indices]\n",
    "    \n",
    "    def get_cluster_summary(self):\n",
    "        \"\"\"\n",
    "        클러스터별 요약 정보 반환\n",
    "        \n",
    "        Returns:\n",
    "            클러스터 요약 정보 딕셔너리\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            summary[cluster_id] = {\n",
    "                'count': len(indices),\n",
    "                'indices': indices,\n",
    "                'sample_data': self.original_data[indices[0]] if indices else None\n",
    "            }\n",
    "        return summary\n",
    "    \n",
    "    def print_cluster_summary(self, top_n=10):\n",
    "        \"\"\"\n",
    "        클러스터 요약 정보 출력\n",
    "        \n",
    "        Args:\n",
    "            top_n: 출력할 상위 클러스터 개수\n",
    "        \"\"\"\n",
    "        # 클러스터를 크기 순으로 정렬\n",
    "        sorted_clusters = sorted(\n",
    "            self.cluster_to_indices.items(),\n",
    "            key=lambda x: len(x[1]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"클러스터 요약 정보\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"총 클러스터 수: {len(self.cluster_to_indices)}\")\n",
    "        print(f\"총 데이터 수: {len(self.cluster_labels)}\")\n",
    "        print()\n",
    "        \n",
    "        for i, (cluster_id, indices) in enumerate(sorted_clusters[:top_n]):\n",
    "            print(f\"[클러스터 {cluster_id}] 데이터 개수: {len(indices)}\")\n",
    "            if indices:\n",
    "                sample = self.original_data[indices[0]]\n",
    "                # 주요 필드만 출력\n",
    "                if isinstance(sample, dict):\n",
    "                    key_fields = ['processName', 'eventName', 'syscall', 'eventId']\n",
    "                    sample_info = {k: sample.get(k, 'N/A') for k in key_fields if k in sample}\n",
    "                    print(f\"  샘플 데이터: {sample_info}\")\n",
    "                else:\n",
    "                    print(f\"  샘플 데이터: {str(sample)[:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    def get_cluster_statistics(self):\n",
    "        \"\"\"\n",
    "        클러스터 통계 정보 반환 (DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "            클러스터 통계 DataFrame\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            cluster_data = [self.original_data[idx] for idx in indices]\n",
    "            \n",
    "            # 주요 필드별 통계\n",
    "            process_names = [d.get('processName', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            event_names = [d.get('eventName', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            syscalls = [d.get('syscall', 'N/A') for d in cluster_data if isinstance(d, dict)]\n",
    "            \n",
    "            stats.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'count': len(indices),\n",
    "                'unique_processes': len(set(process_names)),\n",
    "                'unique_events': len(set(event_names)),\n",
    "                'unique_syscalls': len(set(syscalls)),\n",
    "                'most_common_process': max(set(process_names), key=process_names.count) if process_names else 'N/A',\n",
    "                'most_common_event': max(set(event_names), key=event_names.count) if event_names else 'N/A',\n",
    "                'most_common_syscall': max(set(syscalls), key=syscalls.count) if syscalls else 'N/A',\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).sort_values('count', ascending=False)\n",
    "    \n",
    "    def export_cluster_mapping(self, output_path='cluster_mapping.json'):\n",
    "        \"\"\"\n",
    "        클러스터 매핑 정보를 JSON 파일로 저장\n",
    "        각 인덱스에 번호와 실제 텍스트 데이터를 포함\n",
    "        \n",
    "        Args:\n",
    "            output_path: 출력 파일 경로\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            'total_clusters': len(self.cluster_to_indices),\n",
    "            'total_samples': len(self.cluster_labels),\n",
    "            'clusters': {}\n",
    "        }\n",
    "        \n",
    "        for cluster_id, indices in self.cluster_to_indices.items():\n",
    "            # 각 인덱스에 대해 번호와 실제 데이터를 매핑\n",
    "            indexed_data = []\n",
    "            for idx in indices:\n",
    "                data = self.original_data[idx]\n",
    "                indexed_data.append({\n",
    "                    'index': idx,\n",
    "                    'data': data\n",
    "                })\n",
    "            \n",
    "            mapping['clusters'][str(cluster_id)] = {\n",
    "                'count': len(indices),\n",
    "                'indices': indices,  # 기존 인덱스 리스트도 유지\n",
    "                'data': indexed_data,  # 번호와 실제 데이터 매핑\n",
    "                'sample_data': self.original_data[indices[0]] if indices else None\n",
    "            }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"클러스터 매핑 정보가 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터링 결과와 원본 데이터 매칭 예시\n",
    "if 'embeddings_tensor' in globals() and 'train_normal_loader' in globals():\n",
    "    # 클러스터링 수행 (.env에서 설정값 사용)\n",
    "    print(\"클러스터링 수행 중...\")\n",
    "    labels, n_clusters, info = cluster_embeddings(\n",
    "        embeddings_tensor,\n",
    "        alpha=CLUSTERING_ALPHA,\n",
    "        method=CLUSTERING_METHOD,\n",
    "        random_state=CLUSTERING_RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n클러스터링 완료!\")\n",
    "    print(f\"클러스터 수: {n_clusters}\")\n",
    "    print(f\"노이즈 포인트: {info.get('n_noise', 0)}\")\n",
    "    print(f\"실루엣 점수: {info['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # ClusterTracer 생성\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"클러스터 추적 객체 생성 중...\")\n",
    "    tracer = ClusterTracer(\n",
    "        dataloader=train_normal_loader,\n",
    "        cluster_labels=labels,\n",
    "        filter_func=filter_text\n",
    "    )\n",
    "    \n",
    "    # 클러스터 요약 정보 출력 (.env에서 설정값 사용)\n",
    "    tracer.print_cluster_summary(top_n=TOP_N_CLUSTERS)\n",
    "    \n",
    "    # 클러스터 통계 정보 (DataFrame)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"클러스터 통계 정보\")\n",
    "    print(\"=\" * 80)\n",
    "    stats_df = tracer.get_cluster_statistics()\n",
    "    print(stats_df.head(15))\n",
    "    \n",
    "    # 특정 클러스터의 데이터 확인 예시\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"특정 클러스터 데이터 확인 예시\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 가장 큰 클러스터 확인\n",
    "    largest_cluster_id = stats_df.iloc[0]['cluster_id']\n",
    "    print(f\"\\n가장 큰 클러스터 ID: {largest_cluster_id}\")\n",
    "    cluster_data = tracer.get_cluster_data(largest_cluster_id)\n",
    "    print(f\"데이터 개수: {len(cluster_data)}\")\n",
    "    print(\"\\n첫 3개 샘플:\")\n",
    "    for i, data in enumerate(cluster_data[:3]):\n",
    "        print(f\"\\n샘플 {i+1}:\")\n",
    "        if isinstance(data, dict):\n",
    "            key_fields = ['processName', 'eventName', 'syscall', 'eventId', 'timestamp']\n",
    "            for field in key_fields:\n",
    "                if field in data:\n",
    "                    print(f\"  {field}: {data[field]}\")\n",
    "    \n",
    "    # 클러스터 매핑 정보 저장\n",
    "    tracer.export_cluster_mapping('cluster_mapping.json')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"사용 가능한 메서드:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. tracer.get_cluster_data(cluster_id) - 특정 클러스터의 모든 데이터 반환\")\n",
    "    print(\"2. tracer.get_cluster_summary() - 클러스터별 요약 정보 딕셔너리\")\n",
    "    print(\"3. tracer.print_cluster_summary(top_n=10) - 클러스터 요약 정보 출력\")\n",
    "    print(\"4. tracer.get_cluster_statistics() - 클러스터 통계 DataFrame\")\n",
    "    print(\"5. tracer.export_cluster_mapping(path) - 클러스터 매핑 정보를 JSON으로 저장\")\n",
    "    \n",
    "else:\n",
    "    print(\"embeddings_tensor 또는 train_normal_loader가 아직 생성되지 않았습니다.\")\n",
    "    print(\"먼저 셀 4를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab12b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def create_ngram_sequence(matched_sequence_file='matched_sequence.json', n=None):\n",
    "    \"\"\"\n",
    "    matched_sequence.json에서 cluster_id를 추출하여 n-gram 시퀀스를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        matched_sequence_file: matched_sequence.json 파일 경로\n",
    "        n: n-gram의 n 값 (예: 2=bigram, 3=trigram)\n",
    "            None이면 전역 N_GRAM_SIZE 사용 (.env에서 로드)\n",
    "    \n",
    "    Returns:\n",
    "        list: n-gram 시퀀스 리스트 (각 항목은 n개의 cluster_id 튜플)\n",
    "    \"\"\"\n",
    "    # n 값이 없으면 전역 N_GRAM_SIZE 사용\n",
    "    if n is None:\n",
    "        n = globals().get('N_GRAM_SIZE', 2)\n",
    "    \n",
    "    # matched_sequence.json 파일 읽기\n",
    "    with open(matched_sequence_file, 'r', encoding='utf-8') as f:\n",
    "        matched_sequence = json.load(f)\n",
    "    \n",
    "    # cluster_id 시퀀스 추출 (인덱스 순서대로 정렬)\n",
    "    cluster_sequence = []\n",
    "    for item in sorted(matched_sequence, key=lambda x: x['index']):\n",
    "        cluster_sequence.append(item['cluster_id'])\n",
    "    \n",
    "    # n-gram 생성\n",
    "    ngrams = []\n",
    "    for i in range(len(cluster_sequence) - n + 1):\n",
    "        ngram = tuple(cluster_sequence[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams, cluster_sequence\n",
    "\n",
    "def analyze_ngrams(ngrams, top_k=10):\n",
    "    \"\"\"\n",
    "    n-gram 통계 분석\n",
    "    \n",
    "    Args:\n",
    "        ngrams: n-gram 리스트\n",
    "        top_k: 출력할 상위 k개 n-gram\n",
    "    \n",
    "    Returns:\n",
    "        dict: 통계 정보\n",
    "    \"\"\"\n",
    "    # n-gram 빈도 계산\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    # 통계 정보\n",
    "    stats = {\n",
    "        'total_ngrams': len(ngrams),\n",
    "        'unique_ngrams': len(ngram_counts),\n",
    "        'top_ngrams': ngram_counts.most_common(top_k),\n",
    "        'ngram_counts': dict(ngram_counts)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# n-gram 시퀀스 생성 (예시: bigram, trigram)\n",
    "print(\"=\" * 80)\n",
    "print(\"N-gram 시퀀스 생성\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# N-gram 생성 (.env에서 설정된 N_GRAM_SIZE 사용)\n",
    "print(f\"\\n1. {N_GRAM_SIZE}-gram 생성 중...\")\n",
    "ngrams, cluster_seq = create_ngram_sequence('matched_sequence.json', n=None)  # None이면 N_GRAM_SIZE 사용\n",
    "print(f\"   총 {len(ngrams)}개의 {N_GRAM_SIZE}-gram 생성됨\")\n",
    "print(f\"   클러스터 시퀀스 길이: {len(cluster_seq)}\")\n",
    "\n",
    "# N-gram 통계\n",
    "print(f\"\\n2. {N_GRAM_SIZE}-gram 통계:\")\n",
    "print(\"-\" * 80)\n",
    "ngram_stats = analyze_ngrams(ngrams, top_k=10)\n",
    "print(f\"   고유 {N_GRAM_SIZE}-gram 수: {ngram_stats['unique_ngrams']}\")\n",
    "print(f\"\\n   상위 10개 빈도 {N_GRAM_SIZE}-gram:\")\n",
    "for i, (ngram, count) in enumerate(ngram_stats['top_ngrams'], 1):\n",
    "    print(f\"   {i:2d}. {ngram}: {count:4d}회 ({count/len(ngrams)*100:.2f}%)\")\n",
    "\n",
    "# 추가: Bigram과 Trigram도 생성 (비교용)\n",
    "print(\"\\n3. 추가 통계 (Bigram, Trigram):\")\n",
    "print(\"-\" * 80)\n",
    "bigrams, _ = create_ngram_sequence('matched_sequence.json', n=2)\n",
    "trigrams, _ = create_ngram_sequence('matched_sequence.json', n=3)\n",
    "\n",
    "bigram_stats = analyze_ngrams(bigrams, top_k=5)\n",
    "trigram_stats = analyze_ngrams(trigrams, top_k=5)\n",
    "\n",
    "print(f\"   Bigram: 총 {len(bigrams)}개, 고유 {bigram_stats['unique_ngrams']}개\")\n",
    "print(f\"   Trigram: 총 {len(trigrams)}개, 고유 {trigram_stats['unique_ngrams']}개\")\n",
    "\n",
    "# 클러스터 시퀀스 샘플 출력\n",
    "print(f\"\\n4. 클러스터 시퀀스 샘플 (처음 20개):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   {cluster_seq[:20]}\")\n",
    "\n",
    "# N-gram 샘플 출력\n",
    "print(f\"\\n5. {N_GRAM_SIZE}-gram 샘플 (처음 10개):\")\n",
    "print(\"-\" * 80)\n",
    "for i, ngram in enumerate(ngrams[:10], 1):\n",
    "    print(f\"   {i:2d}. {ngram}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"N-gram 생성 완료!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n사용 가능한 변수:\")\n",
    "print(f\"  - ngrams: {len(ngrams)}개의 {N_GRAM_SIZE}-gram (기본값, .env에서 설정)\")\n",
    "print(f\"  - bigrams: {len(bigrams)}개의 bigram\")\n",
    "print(f\"  - trigrams: {len(trigrams)}개의 trigram\")\n",
    "print(f\"  - cluster_seq: 원본 클러스터 시퀀스 ({len(cluster_seq)}개)\")\n",
    "print(f\"  - ngram_stats: {N_GRAM_SIZE}-gram 통계 정보\")\n",
    "print(f\"  - bigram_stats: bigram 통계 정보\")\n",
    "print(f\"  - trigram_stats: trigram 통계 정보\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "import json\n",
    "\n",
    "def create_ngram_bloom_filter(ngrams, error_rate=None):\n",
    "    \"\"\"\n",
    "    N-gram 리스트로부터 Bloom filter를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        ngrams: n-gram 리스트 (튜플들의 리스트)\n",
    "        error_rate: False positive 확률 (None이면 전역 BLOOM_FILTER_ERROR_RATE 사용)\n",
    "                    기본값: 0.001 = 0.1%\n",
    "                    범위: 0 < error_rate < 1\n",
    "                    주의: 매우 작은 값(예: 1e-100)은 메모리 사용량을 극도로 증가시킬 수 있음\n",
    "    \n",
    "    Returns:\n",
    "        BloomFilter: 생성된 Bloom filter 객체\n",
    "    \"\"\"\n",
    "    # error_rate가 없으면 전역 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    if error_rate is None:\n",
    "        error_rate = globals().get('BLOOM_FILTER_ERROR_RATE', 0.001)\n",
    "    \n",
    "    # error_rate 유효성 검증\n",
    "    if error_rate <= 0 or error_rate >= 1:\n",
    "        raise ValueError(\n",
    "            f\"error_rate는 0과 1 사이의 값이어야 합니다. 현재 값: {error_rate}\"\n",
    "        )\n",
    "    \n",
    "    # 매우 작은 값에 대한 경고 (1e-10보다 작으면)\n",
    "    if error_rate < 1e-10:\n",
    "        import warnings\n",
    "        warnings.warn(\n",
    "            f\"매우 작은 error_rate ({error_rate})를 사용하면 메모리 사용량이 극도로 증가할 수 있습니다. \"\n",
    "            f\"일반적으로 0.001 (0.1%) 이상의 값을 권장합니다.\",\n",
    "            UserWarning\n",
    "        )\n",
    "    \n",
    "    # 고유 n-gram 개수 계산\n",
    "    unique_ngrams = set(ngrams)\n",
    "    capacity = len(unique_ngrams)\n",
    "    \n",
    "    # Bloom filter 생성\n",
    "    bloom = BloomFilter(capacity=capacity, error_rate=error_rate)\n",
    "    \n",
    "    # 모든 고유 n-gram을 Bloom filter에 추가\n",
    "    for ngram in unique_ngrams:\n",
    "        # 튜플을 문자열로 변환하여 추가\n",
    "        bloom.add(str(ngram))\n",
    "    \n",
    "    return bloom\n",
    "\n",
    "def check_ngram_in_bloom(bloom_filter, ngram):\n",
    "    \"\"\"\n",
    "    N-gram이 Bloom filter에 존재하는지 확인하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        ngram: 확인할 n-gram (튜플)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True면 존재 가능 (False positive 가능), False면 확실히 없음\n",
    "    \"\"\"\n",
    "    return str(ngram) in bloom_filter\n",
    "\n",
    "def check_ngrams_batch(bloom_filter, ngrams):\n",
    "    \"\"\"\n",
    "    여러 N-gram을 한 번에 체크하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        ngrams: 확인할 n-gram 리스트\n",
    "    \n",
    "    Returns:\n",
    "        list: 각 n-gram의 존재 여부 (True/False)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for ngram in ngrams:\n",
    "        results.append(check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    return results\n",
    "\n",
    "def analyze_bloom_filter_performance(bloom_filter, known_ngrams, test_ngrams):\n",
    "    \"\"\"\n",
    "    Bloom filter의 성능을 분석하는 함수\n",
    "    \n",
    "    Args:\n",
    "        bloom_filter: BloomFilter 객체\n",
    "        known_ngrams: Bloom filter에 추가된 n-gram 리스트 (실제로 존재해야 함)\n",
    "        test_ngrams: 테스트할 n-gram 리스트\n",
    "    \n",
    "    Returns:\n",
    "        dict: 성능 통계\n",
    "    \"\"\"\n",
    "    known_set = set(known_ngrams)\n",
    "    test_set = set(test_ngrams)\n",
    "    \n",
    "    # True Positive: 실제 존재하고 Bloom filter도 True\n",
    "    tp = sum(1 for ngram in test_ngrams if ngram in known_set and check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # False Positive: 실제로 없지만 Bloom filter가 True\n",
    "    fp = sum(1 for ngram in test_ngrams if ngram not in known_set and check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # True Negative: 실제로 없고 Bloom filter도 False\n",
    "    tn = sum(1 for ngram in test_ngrams if ngram not in known_set and not check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    # False Negative: 실제로 존재하지만 Bloom filter가 False (이론적으로 발생하지 않아야 함)\n",
    "    fn = sum(1 for ngram in test_ngrams if ngram in known_set and not check_ngram_in_bloom(bloom_filter, ngram))\n",
    "    \n",
    "    total = len(test_ngrams)\n",
    "    \n",
    "    stats = {\n",
    "        'total_tested': total,\n",
    "        'true_positive': tp,\n",
    "        'false_positive': fp,\n",
    "        'true_negative': tn,\n",
    "        'false_negative': fn,\n",
    "        'false_positive_rate': fp / total if total > 0 else 0.0,\n",
    "        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0.0,\n",
    "        'recall': tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# N-gram Bloom Filter 생성 및 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"N-gram Bloom Filter 생성 및 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ngrams가 존재하는지 확인\n",
    "if 'ngrams' in globals() and ngrams:\n",
    "    print(f\"\\n1. Bloom Filter 생성 중...\")\n",
    "    print(f\"   입력 N-gram 개수: {len(ngrams)}\")\n",
    "    print(f\"   고유 N-gram 개수: {len(set(ngrams))}\")\n",
    "    \n",
    "    # Bloom filter 생성 (.env에서 설정된 error_rate 사용)\n",
    "    bloom_filter = create_ngram_bloom_filter(ngrams, error_rate=None)  # None이면 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    print(f\"   Bloom Filter 생성 완료!\")\n",
    "    print(f\"   Bloom Filter 크기: {bloom_filter.capacity}개 용량\")\n",
    "    print(f\"   False Positive Rate: {bloom_filter.error_rate * 100:.3f}%\")\n",
    "    \n",
    "    # 샘플 체크\n",
    "    print(f\"\\n2. 샘플 N-gram 체크:\")\n",
    "    print(\"-\" * 80)\n",
    "    sample_ngrams = ngrams[:10]\n",
    "    for i, ngram in enumerate(sample_ngrams, 1):\n",
    "        exists = check_ngram_in_bloom(bloom_filter, ngram)\n",
    "        print(f\"   {i:2d}. {ngram}: {'존재함' if exists else '존재하지 않음'}\")\n",
    "    \n",
    "    # 성능 분석\n",
    "    print(f\"\\n3. Bloom Filter 성능 분석:\")\n",
    "    print(\"-\" * 80)\n",
    "    # 전체 ngrams를 train/test로 분할 (80/20)\n",
    "    split_idx = int(len(ngrams) * 0.8)\n",
    "    train_ngrams = ngrams[:split_idx]\n",
    "    test_ngrams = ngrams[split_idx:]\n",
    "    \n",
    "    # Train set으로 Bloom filter 재생성 (.env에서 설정된 error_rate 사용)\n",
    "    train_bloom = create_ngram_bloom_filter(train_ngrams, error_rate=None)  # None이면 BLOOM_FILTER_ERROR_RATE 사용\n",
    "    \n",
    "    # Test set으로 성능 측정\n",
    "    performance = analyze_bloom_filter_performance(train_bloom, train_ngrams, test_ngrams)\n",
    "    \n",
    "    print(f\"   테스트 N-gram 개수: {performance['total_tested']}\")\n",
    "    print(f\"   True Positive: {performance['true_positive']}\")\n",
    "    print(f\"   False Positive: {performance['false_positive']} ({performance['false_positive_rate']*100:.3f}%)\")\n",
    "    print(f\"   True Negative: {performance['true_negative']}\")\n",
    "    print(f\"   False Negative: {performance['false_negative']}\")\n",
    "    print(f\"   Precision: {performance['precision']*100:.2f}%\")\n",
    "    print(f\"   Recall: {performance['recall']*100:.2f}%\")\n",
    "    \n",
    "    # 새로운 N-gram 체크 예시\n",
    "    print(f\"\\n4. 새로운 N-gram 체크 예시:\")\n",
    "    print(\"-\" * 80)\n",
    "    # 존재하지 않을 가능성이 높은 새로운 n-gram 생성\n",
    "    max_cluster_id = max([max(ng) for ng in ngrams if ng])\n",
    "    new_ngrams = [\n",
    "        tuple([max_cluster_id + 100 + i for i in range(N_GRAM_SIZE)]),  # 존재하지 않을 n-gram\n",
    "        ngrams[0] if ngrams else tuple([0] * N_GRAM_SIZE),  # 존재하는 n-gram\n",
    "    ]\n",
    "    \n",
    "    for ngram in new_ngrams:\n",
    "        exists = check_ngram_in_bloom(bloom_filter, ngram)\n",
    "        print(f\"   {ngram}: {'존재 가능 (False positive 가능)' if exists else '존재하지 않음'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Bloom Filter 생성 완료!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n사용 가능한 변수:\")\n",
    "    print(f\"  - bloom_filter: 생성된 Bloom filter 객체\")\n",
    "    print(f\"  - train_bloom: Train set으로 생성된 Bloom filter\")\n",
    "    print(f\"\\n사용 예시:\")\n",
    "    print(f\"  - check_ngram_in_bloom(bloom_filter, (0, 1))\")\n",
    "    print(f\"  - check_ngrams_batch(bloom_filter, [(0,1), (1,2), (2,3)])\")\n",
    "    \n",
    "else:\n",
    "    print(\"ngrams 변수가 없습니다. 먼저 셀 8을 실행하여 n-gram을 생성해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine_cluster_mapping 함수는 dataAnalyze.ipynb로 이동되었습니다.\n",
    "# dataAnalyze.ipynb 파일에서 함수를 import하거나 직접 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25019f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d3c0d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
